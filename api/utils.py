import os
import base64
import cv2
import numpy as np
from PIL import Image
import io
import pandas as pd
import re
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("‚ö†Ô∏è google-generativeai kh√¥ng ƒë∆∞·ª£c c√†i ƒë·∫∑t, Gemini s·∫Ω kh√¥ng ho·∫°t ƒë·ªông")

# Fix cho Pillow 10.0+ kh√¥ng c√≤n Image.ANTIALIAS
# EasyOCR v√† m·ªôt s·ªë th∆∞ vi·ªán v·∫´n c·∫ßn ANTIALIAS
if not hasattr(Image, 'ANTIALIAS'):
    Image.ANTIALIAS = Image.LANCZOS

# Load drug database v√† PDF (cached)
_drug_db = None
_drug_db_path = None
_pdf_reader = None
_pdf_path = None
_ocr_reader = None  # EasyOCR reader (cache ƒë·ªÉ kh√¥ng load l·∫°i m·ªói l·∫ßn)

def get_drug_database():
    """Load v√† cache drug database"""
    global _drug_db, _drug_db_path
    
    # Get path relative to project root
    if _drug_db_path is None:
        # Try different possible paths - s·ª≠ d·ª•ng drug_database_refined.csv
        possible_paths = [
            os.path.join(os.path.dirname(__file__), '..', 'Crawldata', 'drug_database_refined.csv'),
            os.path.join(os.getcwd(), 'Crawldata', 'drug_database_refined.csv'),
            '/var/task/Crawldata/drug_database_refined.csv',  # Vercel lambda path
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                _drug_db_path = path
                break
    
    if _drug_db is None and _drug_db_path and os.path.exists(_drug_db_path):
        try:
            _drug_db = pd.read_csv(_drug_db_path)
            print(f"‚úÖ Loaded {len(_drug_db)} drugs from database")
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading database: {e}")
            _drug_db = pd.DataFrame()
    elif _drug_db is None:
        _drug_db = pd.DataFrame()
    
    return _drug_db

def get_pdf_reader():
    """Load v√† cache PDF reader"""
    global _pdf_reader, _pdf_path
    
    if _pdf_path is None:
        # Try different possible paths
        possible_paths = [
            os.path.join(os.path.dirname(__file__), '..', 'Crawldata', 'duoc-thu-quoc-gia-viet-nam-2018.pdf'),
            os.path.join(os.getcwd(), 'Crawldata', 'duoc-thu-quoc-gia-viet-nam-2018.pdf'),
            '/var/task/Crawldata/duoc-thu-quoc-gia-viet-nam-2018.pdf',  # Vercel lambda path
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                _pdf_path = path
                break
    
    if _pdf_reader is None and _pdf_path and os.path.exists(_pdf_path):
        try:
            from pypdf import PdfReader
            _pdf_reader = PdfReader(_pdf_path)
            print(f"‚úÖ Loaded PDF with {len(_pdf_reader.pages)} pages")
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading PDF: {e}")
            _pdf_reader = None
    elif _pdf_reader is None:
        _pdf_reader = None
    
    return _pdf_reader

def decode_base64_image(base64_string):
    """Decode base64 string th√†nh image"""
    try:
        # Remove data URL prefix if present
        if ',' in base64_string:
            base64_string = base64_string.split(',')[1]
        
        image_data = base64.b64decode(base64_string)
        image = Image.open(io.BytesIO(image_data))
        
        # Convert to RGB if necessary
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        return np.array(image)
    except Exception as e:
        print(f"Error decoding image: {e}")
        return None

def preprocess_image(image_array):
    """Ti·ªÅn x·ª≠ l√Ω ·∫£nh ƒë·ªÉ c·∫£i thi·ªán OCR"""
    try:
        # Convert to grayscale
        if len(image_array.shape) == 3:
            gray = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)
        else:
            gray = image_array
        
        # Apply denoising
        denoised = cv2.fastNlMeansDenoising(gray, None, 10, 7, 21)
        
        # Apply threshold
        _, thresh = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        # Resize n·∫øu ·∫£nh qu√° nh·ªè (t·ªëi thi·ªÉu 300px width ƒë·ªÉ OCR t·ªët h∆°n)
        height, width = thresh.shape
        if width < 300:
            scale = 300 / width
            new_width = int(width * scale)
            new_height = int(height * scale)
            thresh = cv2.resize(thresh, (new_width, new_height), interpolation=cv2.INTER_CUBIC)
        
        return thresh
    except Exception as e:
        print(f"Error preprocessing image: {e}")
        return image_array

def get_ocr_reader():
    """L·∫•y ho·∫∑c kh·ªüi t·∫°o EasyOCR reader (cache ƒë·ªÉ kh√¥ng load l·∫°i m·ªói l·∫ßn)"""
    global _ocr_reader
    if _ocr_reader is None:
        try:
            import easyocr
            print("üîÑ ƒêang kh·ªüi t·∫°o EasyOCR (l·∫ßn ƒë·∫ßu c√≥ th·ªÉ m·∫•t v√†i ph√∫t ƒë·ªÉ t·∫£i model)...")
            # H·ªó tr·ª£ ti·∫øng Vi·ªát v√† ti·∫øng Anh
            _ocr_reader = easyocr.Reader(['vi', 'en'], gpu=False)
            print("‚úÖ EasyOCR ƒë√£ s·∫µn s√†ng!")
        except ImportError:
            print("‚ö†Ô∏è EasyOCR ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Ch·∫°y: pip install easyocr")
            return None
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói kh·ªüi t·∫°o EasyOCR: {e}")
            return None
    return _ocr_reader

def extract_text_from_image(image_array):
    """Tr√≠ch xu·∫•t text t·ª´ ·∫£nh s·ª≠ d·ª•ng EasyOCR, tr·∫£ v·ªÅ (selected_text, all_ocr_texts)"""
    try:
        reader = get_ocr_reader()
        if reader is None:
            return None, []
        
        # EasyOCR c·∫ßn ·∫£nh ·ªü d·∫°ng numpy array (BGR ho·∫∑c RGB)
        if len(image_array.shape) == 2:
            image_rgb = cv2.cvtColor(image_array, cv2.COLOR_GRAY2RGB)
        else:
            image_rgb = image_array
        
        # Chuy·ªÉn t·ª´ RGB sang BGR (OpenCV format)
        image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)
        
        # OCR v·ªõi EasyOCR
        results = reader.readtext(image_bgr)
        
        if not results:
            return None, []
        
        # L·∫•y t·∫•t c·∫£ text ƒë√£ nh·∫≠n di·ªán
        all_texts = []
        for (bbox, text, confidence) in results:
            if confidence > 0.3:  # Ch·ªâ l·∫•y text c√≥ ƒë·ªô tin c·∫≠y > 30%
                all_texts.append(text.strip())
        
        if not all_texts:
            return None, []
        
        # ∆Øu ti√™n text d√†i nh·∫•t (th∆∞·ªùng l√† t√™n thu·ªëc)
        selected_text = max(all_texts, key=len) if all_texts else ' '.join(all_texts)
        
        # Tr·∫£ v·ªÅ text ƒë√£ ch·ªçn v√† danh s√°ch t·∫•t c·∫£ text
        return selected_text, all_texts
        
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói OCR: {e}")
        return None, []

def search_drug_in_database(drug_name):
    """T√¨m ki·∫øm thu·ªëc trong database"""
    drug_db = get_drug_database()
    
    if drug_db is None or drug_db.empty:
        return None
    
    # T√¨m ki·∫øm kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng
    drug_name_lower = drug_name.lower().strip()
    
    # T√¨m exact match
    exact_match = drug_db[drug_db['DrugName'].str.lower() == drug_name_lower]
    if not exact_match.empty:
        return exact_match.iloc[0].to_dict()
    
    # T√¨m partial match
    partial_match = drug_db[drug_db['DrugName'].str.lower().str.contains(drug_name_lower, na=False)]
    if not partial_match.empty:
        return partial_match.iloc[0].to_dict()
    
    # T√¨m theo t·ª´ kh√≥a
    keywords = drug_name_lower.split()
    for keyword in keywords:
        if len(keyword) > 3:  # Ch·ªâ t√¨m t·ª´ c√≥ > 3 k√Ω t·ª±
            keyword_match = drug_db[drug_db['DrugName'].str.lower().str.contains(keyword, na=False)]
            if not keyword_match.empty:
                return keyword_match.iloc[0].to_dict()
    
    return None

def summarize_drug_info_with_gemini(pdf_text, drug_name, drug_info):
    """
    S·ª≠ d·ª•ng Gemini AI ƒë·ªÉ ƒë·ªçc to√†n b·ªô th√¥ng tin t·ª´ PDF v√† t·ªïng h·ª£p th√†nh:
    - C√°ch d√πng (usage): D·ªÖ hi·ªÉu, ng·∫Øn g·ªçn
    - L∆∞u √Ω (notes): T·ª´ ch·ªëng ch·ªâ ƒë·ªãnh, t∆∞∆°ng t√°c thu·ªëc, t√°c d·ª•ng ph·ª•
    """
    if not pdf_text or len(pdf_text.strip()) < 50:
        return {'usage': '', 'notes': ''}
    
    if not GEMINI_AVAILABLE:
        return {'usage': '', 'notes': ''}
    
    # L·∫•y API key t·ª´ environment variable
    gemini_api_key = os.getenv('GEMINI_API_KEY')
    if not gemini_api_key:
        print("‚ö†Ô∏è GEMINI_API_KEY kh√¥ng ƒë∆∞·ª£c c·∫•u h√¨nh, tr·∫£ v·ªÅ r·ªóng")
        return {'usage': '', 'notes': ''}
    
    try:
        # C·∫•u h√¨nh Gemini
        genai.configure(api_key=gemini_api_key)
        # S·ª≠ d·ª•ng Gemini 2.0 Flash (model m·ªõi nh·∫•t, nhanh v√† ch√≠nh x√°c)
        try:
            model = genai.GenerativeModel('gemini-2.0-flash-exp')
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ d√πng gemini-2.0-flash-exp, th·ª≠ gemini-2.0-flash: {e}")
            try:
                model = genai.GenerativeModel('gemini-2.0-flash')
            except Exception as e2:
                print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ d√πng gemini-2.0-flash, th·ª≠ gemini-1.5-flash: {e2}")
                model = genai.GenerativeModel('gemini-1.5-flash')
        
        # L·∫•y th√¥ng tin b·ªï sung t·ª´ drug_info
        category = drug_info.get('Category', '')
        active_ingredient = drug_info.get('ActiveIngredient', '')
        
        # Gi·ªõi h·∫°n ƒë·ªô d√†i PDF text ƒë·ªÉ tr√°nh v∆∞·ª£t qu√° token limit
        # TƒÉng l√™n 4000 ƒë·ªÉ c√≥ ƒë·ªß context cho Gemini filter ƒë√∫ng thu·ªëc
        pdf_text_limited = pdf_text[:4000] if len(pdf_text) > 4000 else pdf_text
        
        # Prompt ƒë·ªÉ t·ªïng h·ª£p th√¥ng tin - c·∫£i thi·ªán ƒë·ªÉ filter ƒë√∫ng thu·ªëc
        prompt = f"""B·∫°n l√† m·ªôt d∆∞·ª£c sƒ© chuy√™n nghi·ªáp. H√£y ƒë·ªçc v√† t·ªïng h·ª£p th√¥ng tin t·ª´ D∆∞·ª£c th∆∞ Qu·ªëc gia v·ªÅ thu·ªëc C·ª§ TH·ªÇ sau:

**THU·ªêC C·∫¶N T√åM:**
- T√™n thu·ªëc: {drug_name}
- Ho·∫°t ch·∫•t: {active_ingredient}
- Ph√¢n lo·∫°i: {category}

**L∆ØU √ù QUAN TR·ªåNG:**
- Trang PDF c√≥ th·ªÉ ch·ª©a th√¥ng tin c·ªßa NHI·ªÄU thu·ªëc kh√°c nhau
- B·∫†N CH·ªà ƒê∆Ø·ª¢C t·ªïng h·ª£p th√¥ng tin v·ªÅ thu·ªëc "{drug_name}" ho·∫∑c "{active_ingredient}"
- B·ªé QUA ho√†n to√†n th√¥ng tin v·ªÅ c√°c thu·ªëc kh√°c (nh∆∞ Polymyxin, Polygelin, ho·∫∑c b·∫•t k·ª≥ thu·ªëc n√†o kh√°c)
- N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin v·ªÅ thu·ªëc n√†y, tr·∫£ v·ªÅ "Kh√¥ng t√¨m th·∫•y th√¥ng tin" thay v√¨ th√¥ng tin c·ªßa thu·ªëc kh√°c

**Th√¥ng tin t·ª´ D∆∞·ª£c th∆∞ (c√≥ th·ªÉ ch·ª©a nhi·ªÅu thu·ªëc):**
{pdf_text_limited}

**Y√äU C·∫¶U:**
1. T·ªïng h·ª£p ph·∫ßn "C√ÅCH D√ôNG" (usage) - CH·ªà v·ªÅ thu·ªëc "{drug_name}":
   - Vi·∫øt b·∫±ng ng√¥n ng·ªØ ƒë∆°n gi·∫£n, d·ªÖ hi·ªÉu
   - T·∫≠p trung v√†o: li·ªÅu l∆∞·ª£ng, th·ªùi ƒëi·ªÉm u·ªëng, c√°ch u·ªëng, t·∫ßn su·∫•t
   - S·ª≠ d·ª•ng c√¢u ng·∫Øn g·ªçn, r√µ r√†ng
   - Lo·∫°i b·ªè thu·∫≠t ng·ªØ y khoa ph·ª©c t·∫°p
   - N·∫øu kh√¥ng c√≥ th√¥ng tin, vi·∫øt: "Th√¥ng tin c√°ch d√πng kh√¥ng c√≥ trong d∆∞·ª£c th∆∞"

2. T·ªïng h·ª£p ph·∫ßn "L∆ØU √ù" (notes) - CH·ªà v·ªÅ thu·ªëc "{drug_name}":
   - T·ª´ ch·ªëng ch·ªâ ƒë·ªãnh: ai kh√¥ng n√™n d√πng
   - T∆∞∆°ng t√°c thu·ªëc: kh√¥ng d√πng c√πng v·ªõi thu·ªëc g√¨
   - T√°c d·ª•ng ph·ª•: c·∫ßn ch√∫ √Ω g√¨
   - ƒê·ªëi t∆∞·ª£ng ƒë·∫∑c bi·ªát: ph·ª• n·ªØ c√≥ thai, tr·∫ª em, ng∆∞·ªùi gi√†
   - B·∫£o qu·∫£n: c√°ch b·∫£o qu·∫£n thu·ªëc
   - N·∫øu kh√¥ng c√≥ th√¥ng tin, vi·∫øt: "Th√¥ng tin l∆∞u √Ω kh√¥ng c√≥ trong d∆∞·ª£c th∆∞"

**Tr·∫£ v·ªÅ theo ƒë·ªãnh d·∫°ng JSON:**
{{
  "usage": "Ph·∫ßn c√°ch d√πng ƒë√£ t·ªïng h·ª£p (CH·ªà v·ªÅ {drug_name})",
  "notes": "Ph·∫ßn l∆∞u √Ω ƒë√£ t·ªïng h·ª£p (CH·ªà v·ªÅ {drug_name})"
}}

**QUAN TR·ªåNG:** Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng th√™m text kh√°c. KH√îNG ƒë∆∞·ª£c tr·∫£ v·ªÅ th√¥ng tin c·ªßa thu·ªëc kh√°c."""
        
        response = model.generate_content(prompt)
        result_text = response.text.strip()
        
        # Lo·∫°i b·ªè markdown code blocks n·∫øu c√≥
        result_text = result_text.replace('```json', '').replace('```', '').strip()
        
        # Parse JSON
        import json
        try:
            result = json.loads(result_text)
            usage = result.get('usage', '').strip()
            notes = result.get('notes', '').strip()
            
            # Ki·ªÉm tra xem c√≥ ph·∫£i l√† th√¥ng b√°o l·ªói kh√¥ng
            if 'kh√¥ng t√¨m th·∫•y' in usage.lower() or 'kh√¥ng c√≥ trong' in usage.lower():
                usage = "Th√¥ng tin c√°ch d√πng kh√¥ng c√≥ trong d∆∞·ª£c th∆∞ cho thu·ªëc n√†y."
            if 'kh√¥ng t√¨m th·∫•y' in notes.lower() or 'kh√¥ng c√≥ trong' in notes.lower():
                notes = "Th√¥ng tin l∆∞u √Ω kh√¥ng c√≥ trong d∆∞·ª£c th∆∞ cho thu·ªëc n√†y."
            
            # Gi·ªõi h·∫°n ƒë·ªô d√†i
            if len(usage) > 500:
                usage = usage[:500] + "..."
            if len(notes) > 600:
                notes = notes[:600] + "..."
            
            print(f"‚úÖ ƒê√£ t·ªïng h·ª£p th√¥ng tin v·ªõi Gemini cho {drug_name}")
            return {
                'usage': usage,
                'notes': notes
            }
        except json.JSONDecodeError:
            # N·∫øu kh√¥ng parse ƒë∆∞·ª£c JSON, th·ª≠ extract th·ªß c√¥ng
            print("‚ö†Ô∏è Kh√¥ng parse ƒë∆∞·ª£c JSON t·ª´ Gemini, th·ª≠ extract th·ªß c√¥ng")
            # T√¨m ph·∫ßn usage v√† notes trong text
            usage_start = result_text.find('"usage"') or result_text.find('C√ÅCH D√ôNG')
            notes_start = result_text.find('"notes"') or result_text.find('L∆ØU √ù')
            
            if usage_start > -1 and notes_start > -1:
                usage = result_text[usage_start:notes_start].replace('"usage":', '').strip('",')
                notes = result_text[notes_start:].replace('"notes":', '').strip('",')
                return {'usage': usage[:500], 'notes': notes[:600]}
            else:
                # Fallback: chia text l√†m 2 ph·∫ßn
                parts = result_text.split('\n\n')
                usage = parts[0] if len(parts) > 0 else ''
                notes = parts[1] if len(parts) > 1 else ''
                return {'usage': usage[:500], 'notes': notes[:600]}
        
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi g·ªçi Gemini API: {e}")
        return {'usage': '', 'notes': ''}

def generate_recommendations(drug_info, pdf_details):
    """
    T·∫°o khuy·∫øn ngh·ªã s·ª≠ d·ª•ng thu·ªëc d·ª±a tr√™n th√¥ng tin thu·ªëc
    """
    recommendations = []
    
    # Khuy·∫øn ngh·ªã d·ª±a tr√™n ph√¢n lo·∫°i
    category = drug_info.get('Category', '').lower()
    if 'kh√°ng sinh' in category:
        recommendations.append("Kh√°ng sinh c·∫ßn u·ªëng ƒë·ªß li·ªÅu v√† ƒë·ªß th·ªùi gian theo ch·ªâ ƒë·ªãnh c·ªßa b√°c sƒ©, kh√¥ng t·ª± √Ω ng·ª´ng thu·ªëc.")
    elif 'gi·∫£m ƒëau' in category or 'h·∫° s·ªët' in category:
        recommendations.append("Thu·ªëc gi·∫£m ƒëau h·∫° s·ªët n√™n u·ªëng sau khi ƒÉn ƒë·ªÉ tr√°nh k√≠ch ·ª©ng d·∫° d√†y.")
    elif 'ch·ªëng vi√™m' in category:
        recommendations.append("Thu·ªëc ch·ªëng vi√™m n√™n u·ªëng sau khi ƒÉn v√† u·ªëng nhi·ªÅu n∆∞·ªõc.")
    elif 'vitamin' in category or 'b·ªï sung' in category:
        recommendations.append("Vitamin v√† ch·∫•t b·ªï sung n√™n u·ªëng theo li·ªÅu l∆∞·ª£ng khuy·∫øn ngh·ªã, kh√¥ng l·∫°m d·ª•ng.")
    
    # Khuy·∫øn ngh·ªã d·ª±a tr√™n ch·ªëng ch·ªâ ƒë·ªãnh
    contraindications = pdf_details.get('contraindications', '').lower()
    if contraindications:
        if 'ph·ª• n·ªØ c√≥ thai' in contraindications or 'mang thai' in contraindications:
            recommendations.append("Kh√¥ng s·ª≠ d·ª•ng cho ph·ª• n·ªØ c√≥ thai ho·∫∑c ƒëang cho con b√∫ n·∫øu kh√¥ng c√≥ ch·ªâ ƒë·ªãnh c·ªßa b√°c sƒ©.")
        if 'tr·∫ª em' in contraindications or 'tr·∫ª nh·ªè' in contraindications:
            recommendations.append("C·∫ßn th·∫≠n tr·ªçng khi s·ª≠ d·ª•ng cho tr·∫ª em, n√™n tham kh·∫£o √Ω ki·∫øn b√°c sƒ©.")
    
    # Khuy·∫øn ngh·ªã d·ª±a tr√™n c√°ch d√πng
    usage = pdf_details.get('usage', '') or pdf_details.get('dosage', '')
    if usage:
        if 'sau khi ƒÉn' in usage.lower() or 'sau b·ªØa ƒÉn' in usage.lower():
            recommendations.append("N√™n u·ªëng thu·ªëc sau khi ƒÉn ƒë·ªÉ ƒë·∫°t hi·ªáu qu·∫£ t·ªët nh·∫•t v√† gi·∫£m t√°c d·ª•ng ph·ª•.")
        if 'tr∆∞·ªõc khi ƒÉn' in usage.lower() or 'khi ƒë√≥i' in usage.lower():
            recommendations.append("N√™n u·ªëng thu·ªëc tr∆∞·ªõc khi ƒÉn ho·∫∑c khi ƒë√≥i ƒë·ªÉ h·∫•p thu t·ªët h∆°n.")
    
    # Khuy·∫øn ngh·ªã chung
    if not recommendations:
        recommendations.append("Vui l√≤ng ƒë·ªçc k·ªπ h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng tr∆∞·ªõc khi d√πng v√† tu√¢n th·ªß li·ªÅu l∆∞·ª£ng khuy·∫øn ngh·ªã.")
        recommendations.append("N·∫øu c√≥ b·∫•t k·ª≥ d·∫•u hi·ªáu b·∫•t th∆∞·ªùng n√†o, h√£y ng·ª´ng s·ª≠ d·ª•ng v√† tham kh·∫£o √Ω ki·∫øn b√°c sƒ©.")
    else:
        recommendations.append("N·∫øu c√≥ b·∫•t k·ª≥ d·∫•u hi·ªáu b·∫•t th∆∞·ªùng n√†o, h√£y ng·ª´ng s·ª≠ d·ª•ng v√† tham kh·∫£o √Ω ki·∫øn b√°c sƒ©.")
    
    return recommendations

def extract_drug_details_from_pdf(page_number, offset=-1):
    """
    Tr√≠ch xu·∫•t th√¥ng tin chi ti·∫øt t·ª´ PDF d·ª±a tr√™n s·ªë trang
    T√¨m th√†nh ph·∫ßn, c√¥ng d·ª•ng, ch·ªâ ƒë·ªãnh, ch·ªëng ch·ªâ ƒë·ªãnh...
    """
    pdf_reader = get_pdf_reader()
    
    if pdf_reader is None:
        return {}
    
    try:
        # Chuy·ªÉn ƒë·ªïi s·ªë trang s√°ch th√†nh index PDF (pypdf ƒë√°nh s·ªë t·ª´ 0)
        pdf_page_index = int(page_number) + offset - 1
        
        # ƒê·∫£m b·∫£o index h·ª£p l·ªá
        if pdf_page_index < 0 or pdf_page_index >= len(pdf_reader.pages):
            # Th·ª≠ kh√¥ng c√≥ offset
            pdf_page_index = int(page_number) - 1
            if pdf_page_index < 0 or pdf_page_index >= len(pdf_reader.pages):
                return {}
        
        # ƒê·ªçc trang PDF
        page = pdf_reader.pages[pdf_page_index]
        text = page.extract_text()
        
        if not text:
            return {}
        
        details = {
            'composition': '',
            'indications': '',
            'contraindications': '',
            'dosage': '',
            'usage': '',  # C√°ch d√πng
            'full_text': text[:2000]  # Gi·ªõi h·∫°n ƒë·ªÉ tr√°nh qu√° d√†i
        }
        
        # Regex patterns ƒë·ªÉ t√¨m c√°c th√¥ng tin
        patterns = {
            'composition': re.compile(r'(Th√†nh ph·∫ßn|Th√†nh ph·∫ßn ch√≠nh|Ho·∫°t ch·∫•t)[:\.]\s*(.+?)(?:\n|$)', re.IGNORECASE),
            'indications': re.compile(r'(Ch·ªâ ƒë·ªãnh|C√¥ng d·ª•ng|T√°c d·ª•ng)[:\.]\s*(.+?)(?:\n|Ch·ªëng ch·ªâ ƒë·ªãnh|Li·ªÅu d√πng|$)', re.IGNORECASE | re.DOTALL),
            'contraindications': re.compile(r'(Ch·ªëng ch·ªâ ƒë·ªãnh|Kh√¥ng d√πng)[:\.]\s*(.+?)(?:\n|Li·ªÅu d√πng|C√°ch d√πng|$)', re.IGNORECASE | re.DOTALL),
            'dosage': re.compile(r'(Li·ªÅu d√πng|C√°ch d√πng|Li·ªÅu l∆∞·ª£ng|C√°ch s·ª≠ d·ª•ng)[:\.]\s*(.+?)(?:\n|T√°c d·ª•ng ph·ª•|L∆∞u √Ω|B·∫£o qu·∫£n|$)', re.IGNORECASE | re.DOTALL),
            'usage': re.compile(r'(C√°ch d√πng|H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng|S·ª≠ d·ª•ng)[:\.]\s*(.+?)(?:\n|L∆∞u √Ω|T√°c d·ª•ng ph·ª•|$)', re.IGNORECASE | re.DOTALL)
        }
        
        # T√¨m t·ª´ng lo·∫°i th√¥ng tin
        for key, pattern in patterns.items():
            match = pattern.search(text)
            if match:
                details[key] = match.group(2).strip()[:500]  # Gi·ªõi h·∫°n ƒë·ªô d√†i
        
        # N·∫øu kh√¥ng t√¨m th·∫•y "usage", d√πng "dosage" l√†m c√°ch d√πng
        if not details['usage'] and details['dosage']:
            details['usage'] = details['dosage']
        
        return details
        
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói ƒë·ªçc PDF trang {page_number}: {e}")
        return {}

