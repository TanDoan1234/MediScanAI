from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import base64
import cv2
import numpy as np
from PIL import Image
import io
import pandas as pd
import re
from werkzeug.utils import secure_filename
from pypdf import PdfReader

# Fix cho Pillow 10.0+ kh√¥ng c√≤n Image.ANTIALIAS
# EasyOCR v√† m·ªôt s·ªë th∆∞ vi·ªán v·∫´n c·∫ßn ANTIALIAS
if not hasattr(Image, 'ANTIALIAS'):
    Image.ANTIALIAS = Image.LANCZOS

app = Flask(__name__)
CORS(app)  # Cho ph√©p frontend g·ªçi API

# C·∫•u h√¨nh
UPLOAD_FOLDER = 'uploads'
ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif', 'webp'}

if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size

# Load drug database v√† PDF
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DRUG_DB_PATH = os.path.join(BASE_DIR, '..', 'Crawldata', 'drug_database_refined.csv')
PDF_PATH = os.path.join(BASE_DIR, '..', 'Crawldata', 'duoc-thu-quoc-gia-viet-nam-2018.pdf')
drug_db = None
pdf_reader = None
ocr_reader = None  # EasyOCR reader (cache ƒë·ªÉ kh√¥ng load l·∫°i m·ªói l·∫ßn)

def load_drug_database():
    """Load drug database t·ª´ CSV file"""
    global drug_db
    try:
        if os.path.exists(DRUG_DB_PATH):
            drug_db = pd.read_csv(DRUG_DB_PATH)
            print(f"‚úÖ ƒê√£ load {len(drug_db)} thu·ªëc t·ª´ database")
        else:
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file database t·∫°i: {DRUG_DB_PATH}")
            drug_db = pd.DataFrame()
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ load database: {e}")
        drug_db = pd.DataFrame()

def load_pdf():
    """Load PDF d∆∞·ª£c th∆∞ qu·ªëc gia"""
    global pdf_reader
    try:
        if os.path.exists(PDF_PATH):
            pdf_reader = PdfReader(PDF_PATH)
            print(f"‚úÖ ƒê√£ load PDF v·ªõi {len(pdf_reader.pages)} trang")
        else:
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file PDF t·∫°i: {PDF_PATH}")
            pdf_reader = None
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ load PDF: {e}")
        pdf_reader = None

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def decode_base64_image(base64_string):
    """Decode base64 string th√†nh image"""
    try:
        # Remove data URL prefix if present
        if ',' in base64_string:
            base64_string = base64_string.split(',')[1]
        
        image_data = base64.b64decode(base64_string)
        image = Image.open(io.BytesIO(image_data))
        
        # Convert to RGB if necessary
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        return np.array(image)
    except Exception as e:
        print(f"Error decoding image: {e}")
        return None

def preprocess_image(image_array):
    """Ti·ªÅn x·ª≠ l√Ω ·∫£nh ƒë·ªÉ c·∫£i thi·ªán OCR"""
    # Convert to grayscale
    if len(image_array.shape) == 3:
        gray = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)
    else:
        gray = image_array
    
    # Apply denoising
    denoised = cv2.fastNlMeansDenoising(gray, None, 10, 7, 21)
    
    # Apply threshold
    _, thresh = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    
    # Resize n·∫øu ·∫£nh qu√° nh·ªè (t·ªëi thi·ªÉu 300px width ƒë·ªÉ OCR t·ªët h∆°n)
    height, width = thresh.shape
    if width < 300:
        scale = 300 / width
        new_width = int(width * scale)
        new_height = int(height * scale)
        thresh = cv2.resize(thresh, (new_width, new_height), interpolation=cv2.INTER_CUBIC)
    
    return thresh

def get_ocr_reader():
    """L·∫•y ho·∫∑c kh·ªüi t·∫°o EasyOCR reader (cache ƒë·ªÉ kh√¥ng load l·∫°i m·ªói l·∫ßn)"""
    global ocr_reader
    if ocr_reader is None:
        try:
            import easyocr
            print("üîÑ ƒêang kh·ªüi t·∫°o EasyOCR (l·∫ßn ƒë·∫ßu c√≥ th·ªÉ m·∫•t v√†i ph√∫t ƒë·ªÉ t·∫£i model)...")
            # H·ªó tr·ª£ ti·∫øng Vi·ªát v√† ti·∫øng Anh
            ocr_reader = easyocr.Reader(['vi', 'en'], gpu=False)
            print("‚úÖ EasyOCR ƒë√£ s·∫µn s√†ng!")
        except ImportError:
            print("‚ö†Ô∏è EasyOCR ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Ch·∫°y: pip install easyocr")
            return None
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói kh·ªüi t·∫°o EasyOCR: {e}")
            return None
    return ocr_reader

def extract_text_from_image(image_array):
    """Tr√≠ch xu·∫•t text t·ª´ ·∫£nh s·ª≠ d·ª•ng EasyOCR, tr·∫£ v·ªÅ t·∫•t c·∫£ text v√† text ƒë∆∞·ª£c ch·ªçn"""
    try:
        reader = get_ocr_reader()
        if reader is None:
            return None, []
        
        # EasyOCR c·∫ßn ·∫£nh ·ªü d·∫°ng numpy array (BGR ho·∫∑c RGB)
        if len(image_array.shape) == 2:
            image_rgb = cv2.cvtColor(image_array, cv2.COLOR_GRAY2RGB)
        else:
            image_rgb = image_array
        
        # Chuy·ªÉn t·ª´ RGB sang BGR (OpenCV format)
        image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)
        height, width = image_bgr.shape[:2]
        
        # OCR v·ªõi EasyOCR
        results = reader.readtext(image_bgr)
        
        if not results:
            return None, []
        
        # Danh s√°ch t·ª´ th√¥ng th∆∞·ªùng c·∫ßn lo·∫°i b·ªè (kh√¥ng ph·∫£i t√™n thu·ªëc)
        common_words = {
            'arthritis', 'pain', 'relief', 'fever', 'reducer', 'temporary', 'minor',
            'tablets', 'caplets', 'mg', 'each', 'extended', 'release', 'acetaminophen',
            'ibuprofen', 'aspirin', 'do', 'not', 'use', 'with', 'other', 'medicines',
            'containing', 'to', 'open', 'push', 'turn', 'cap', 'warnings', 'directions',
            'store', 'at', 'room', 'temperature', 'keep', 'out', 'of', 'reach', 'children',
            'active', 'ingredient', 'inactive', 'ingredients', 'see', 'package', 'insert'
        }
        
        # L·∫•y t·∫•t c·∫£ text v·ªõi th√¥ng tin chi ti·∫øt
        all_texts = []
        candidate_texts = []
        
        for (bbox, text, confidence) in results:
            if confidence > 0.3:  # Ch·ªâ l·∫•y text c√≥ ƒë·ªô tin c·∫≠y > 30%
                # L√†m s·∫°ch text: lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát ·ªü ƒë·∫ßu/cu·ªëi
                text_cleaned = text.strip()
                # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát th∆∞·ªùng g·∫∑p trong OCR
                text_cleaned = text_cleaned.strip('[](){}.,;:!?-_=+')
                text_clean = text_cleaned.upper()
                
                # B·ªè qua n·∫øu text qu√° ng·∫Øn sau khi l√†m s·∫°ch
                if len(text_cleaned) < 2:
                    continue
                
                # T√≠nh to√°n v·ªã tr√≠ trung t√¢m c·ªßa text
                x_coords = [point[0] for point in bbox]
                y_coords = [point[1] for point in bbox]
                center_x = sum(x_coords) / len(x_coords)
                center_y = sum(y_coords) / len(y_coords)
                
                # T√≠nh kho·∫£ng c√°ch t·ª´ trung t√¢m ·∫£nh (∆∞u ti√™n text ·ªü gi·ªØa)
                distance_from_center = ((center_x - width/2)**2 + (center_y - height/2)**2)**0.5
                normalized_distance = distance_from_center / ((width/2)**2 + (height/2)**2)**0.5
                
                # T√≠nh k√≠ch th∆∞·ªõc text (∆∞u ti√™n text l·ªõn)
                text_width = max(x_coords) - min(x_coords)
                text_height = max(y_coords) - min(y_coords)
                text_area = text_width * text_height
                normalized_area = text_area / (width * height)
                
                # Lo·∫°i b·ªè t·ª´ th√¥ng th∆∞·ªùng
                words = text_clean.split()
                is_common = all(word in common_words for word in words if len(word) > 2)
                
                all_texts.append({
                    'text': text_cleaned,  # D√πng text ƒë√£ l√†m s·∫°ch
                    'confidence': confidence,
                    'center_x': center_x,
                    'center_y': center_y,
                    'distance_from_center': normalized_distance,
                    'area': normalized_area
                })
                
                # Ch·ªâ th√™m v√†o candidate n·∫øu kh√¥ng ph·∫£i t·ª´ th√¥ng th∆∞·ªùng
                if not is_common and len(text_clean) >= 3:
                    # T√≠nh ƒëi·ªÉm ∆∞u ti√™n: confidence cao, ·ªü gi·ªØa, k√≠ch th∆∞·ªõc l·ªõn, kh√¥ng ph·∫£i s·ªë thu·∫ßn t√∫y
                    is_number_only = text_clean.replace(' ', '').replace('.', '').isdigit()
                    if not is_number_only:
                        score = (
                            confidence * 0.4 +  # 40% t·ª´ confidence
                            (1 - normalized_distance) * 0.3 +  # 30% t·ª´ v·ªã tr√≠ (g·∫ßn trung t√¢m h∆°n = t·ªët h∆°n)
                            normalized_area * 0.2 +  # 20% t·ª´ k√≠ch th∆∞·ªõc
                            min(len(text_clean) / 20, 1) * 0.1  # 10% t·ª´ ƒë·ªô d√†i (∆∞u ti√™n text v·ª´a ph·∫£i)
                        )
                        candidate_texts.append({
                            'text': text,
                            'score': score,
                            'confidence': confidence
                        })
        
        if not all_texts:
            return None, []
        
        # S·∫Øp x·∫øp candidate theo ƒëi·ªÉm s·ªë
        candidate_texts.sort(key=lambda x: x['score'], reverse=True)
        
        # C·∫£i thi·ªán: K·∫øt h·ª£p c√°c text g·∫ßn nhau th√†nh t√™n thu·ªëc ƒë·∫ßy ƒë·ªß
        # T√¨m c√°c text c√≥ th·ªÉ l√† m·ªôt ph·∫ßn c·ªßa t√™n thu·ªëc (c√≥ s·ªë %, ch·ªØ hoa, v.v.)
        combined_candidates = []
        
        # ∆Øu ti√™n text c√≥ ch·ª©a s·ªë ph·∫ßn trƒÉm (%)
        percent_texts = [t for t in all_texts if '%' in t['text'] or '10' in t['text'] or '20' in t['text']]
        
        # T√¨m text c√≥ v·∫ª l√† t√™n thu·ªëc (ch·ªØ hoa, kh√¥ng ph·∫£i t·ª´ th√¥ng th∆∞·ªùng)
        drug_name_candidates = []
        for t in all_texts:
            text_upper = t['text'].upper()
            # Lo·∫°i b·ªè text ch·ªâ l√† s·ªë ho·∫∑c qu√° ng·∫Øn
            if len(text_upper) >= 3 and not text_upper.replace(' ', '').replace('.', '').replace('%', '').isdigit():
                # Ki·ªÉm tra xem c√≥ ph·∫£i t·ª´ th√¥ng th∆∞·ªùng kh√¥ng
                words = text_upper.split()
                is_common = any(word in common_words for word in words if len(word) > 2)
                if not is_common:
                    drug_name_candidates.append(t)
        
        # K·∫øt h·ª£p text: ∆Øu ti√™n text c√≥ %, sau ƒë√≥ k·∫øt h·ª£p v·ªõi text kh√°c g·∫ßn nhau
        if percent_texts and drug_name_candidates:
            # T√¨m text g·∫ßn v·ªõi text c√≥ %
            for percent_text in percent_texts:
                percent_center = (percent_text['center_x'], percent_text['center_y'])
                # T√¨m text g·∫ßn nh·∫•t (trong v√≤ng 200px)
                nearby_texts = []
                for candidate in drug_name_candidates:
                    distance = ((candidate['center_x'] - percent_center[0])**2 + 
                               (candidate['center_y'] - percent_center[1])**2)**0.5
                    if distance < 200:  # Text trong v√≤ng 200px
                        nearby_texts.append((candidate, distance))
                
                # S·∫Øp x·∫øp theo kho·∫£ng c√°ch
                nearby_texts.sort(key=lambda x: x[1])
                
                # K·∫øt h·ª£p text g·∫ßn nhau
                if nearby_texts:
                    combined = [percent_text['text']]
                    for candidate, _ in nearby_texts[:2]:  # L·∫•y t·ªëi ƒëa 2 text g·∫ßn nh·∫•t
                        if candidate['text'] not in combined:
                            combined.append(candidate['text'])
                    combined_text = ' '.join(combined)
                    combined_candidates.append({
                        'text': combined_text,
                        'score': 0.9,  # ƒêi·ªÉm cao cho text k·∫øt h·ª£p
                        'confidence': min(percent_text['confidence'], nearby_texts[0][0]['confidence'])
                    })
        
        # N·∫øu c√≥ text k·∫øt h·ª£p, ∆∞u ti√™n n√≥
        if combined_candidates:
            combined_candidates.sort(key=lambda x: x['score'], reverse=True)
            selected_text = combined_candidates[0]['text']
        elif candidate_texts:
            selected_text = candidate_texts[0]['text']
        else:
            # N·∫øu kh√¥ng c√≥ candidate, ch·ªçn text d√†i nh·∫•t kh√¥ng ph·∫£i t·ª´ th√¥ng th∆∞·ªùng
            filtered = [t for t in all_texts if not any(w in common_words for w in t['text'].upper().split())]
            if filtered:
                selected_text = max(filtered, key=lambda x: len(x['text']))['text']
            else:
                selected_text = max(all_texts, key=lambda x: len(x['text']))['text']
        
        # Tr·∫£ v·ªÅ text ƒë√£ ch·ªçn v√† danh s√°ch t·∫•t c·∫£ text
        all_texts_list = [t['text'] for t in all_texts]
        return selected_text, all_texts_list
        
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói OCR: {e}")
        return None, []

def search_drug_in_database(drug_name):
    """T√¨m ki·∫øm thu·ªëc trong database"""
    if drug_db is None or drug_db.empty:
        return None
    
    # L√†m s·∫°ch text: lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát c√≥ th·ªÉ g√¢y l·ªói regex
    drug_name_clean = drug_name.strip()
    # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát ·ªü ƒë·∫ßu/cu·ªëi nh∆∞ [ ] ( ) { }
    drug_name_clean = drug_name_clean.strip('[](){}')
    drug_name_lower = drug_name_clean.lower().strip()
    
    if not drug_name_lower:
        return None
    
    # T√¨m exact match
    exact_match = drug_db[drug_db['DrugName'].str.lower() == drug_name_lower]
    if not exact_match.empty:
        return exact_match.iloc[0].to_dict()
    
    # T√¨m partial match - d√πng regex=False ƒë·ªÉ tr√°nh l·ªói v·ªõi k√Ω t·ª± ƒë·∫∑c bi·ªát
    try:
        partial_match = drug_db[drug_db['DrugName'].str.lower().str.contains(drug_name_lower, na=False, regex=False)]
        if not partial_match.empty:
            return partial_match.iloc[0].to_dict()
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói t√¨m ki·∫øm partial match: {e}")
        # Fallback: escape regex special characters
        import re
        escaped_pattern = re.escape(drug_name_lower)
        try:
            partial_match = drug_db[drug_db['DrugName'].str.lower().str.contains(escaped_pattern, na=False, regex=True)]
            if not partial_match.empty:
                return partial_match.iloc[0].to_dict()
        except:
            pass
    
    # T√¨m theo t·ª´ kh√≥a
    keywords = drug_name_lower.split()
    for keyword in keywords:
        if len(keyword) > 3:  # Ch·ªâ t√¨m t·ª´ c√≥ > 3 k√Ω t·ª±
            # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát t·ª´ keyword
            keyword_clean = keyword.strip('[](){}.,;:!?')
            if len(keyword_clean) > 3:
                try:
                    keyword_match = drug_db[drug_db['DrugName'].str.lower().str.contains(keyword_clean, na=False, regex=False)]
                    if not keyword_match.empty:
                        return keyword_match.iloc[0].to_dict()
                except Exception as e:
                    print(f"‚ö†Ô∏è L·ªói t√¨m ki·∫øm keyword '{keyword_clean}': {e}")
                    continue
    
    return None

def extract_drug_details_from_pdf(page_number, offset=-1):
    """
    Tr√≠ch xu·∫•t th√¥ng tin chi ti·∫øt t·ª´ PDF d·ª±a tr√™n s·ªë trang
    T√¨m th√†nh ph·∫ßn, c√¥ng d·ª•ng, ch·ªâ ƒë·ªãnh, ch·ªëng ch·ªâ ƒë·ªãnh...
    """
    if pdf_reader is None:
        return {}
    
    try:
        # Chuy·ªÉn ƒë·ªïi s·ªë trang s√°ch th√†nh index PDF (pypdf ƒë√°nh s·ªë t·ª´ 0)
        # Offset th∆∞·ªùng l√† -1 v√¨ PDF c√≥ th·ªÉ c√≥ trang b√¨a, m·ª•c l·ª•c
        pdf_page_index = int(page_number) + offset - 1
        
        # ƒê·∫£m b·∫£o index h·ª£p l·ªá
        if pdf_page_index < 0 or pdf_page_index >= len(pdf_reader.pages):
            # Th·ª≠ kh√¥ng c√≥ offset
            pdf_page_index = int(page_number) - 1
            if pdf_page_index < 0 or pdf_page_index >= len(pdf_reader.pages):
                return {}
        
        # ƒê·ªçc trang PDF
        page = pdf_reader.pages[pdf_page_index]
        text = page.extract_text()
        
        if not text:
            return {}
        
        details = {
            'composition': '',
            'indications': '',
            'contraindications': '',
            'dosage': '',
            'full_text': text[:2000]  # Gi·ªõi h·∫°n ƒë·ªÉ tr√°nh qu√° d√†i
        }
        
        # Regex patterns ƒë·ªÉ t√¨m c√°c th√¥ng tin
        patterns = {
            'composition': re.compile(r'(Th√†nh ph·∫ßn|Th√†nh ph·∫ßn ch√≠nh|Ho·∫°t ch·∫•t)[:\.]\s*(.+?)(?:\n|$)', re.IGNORECASE),
            'indications': re.compile(r'(Ch·ªâ ƒë·ªãnh|C√¥ng d·ª•ng|T√°c d·ª•ng)[:\.]\s*(.+?)(?:\n|Ch·ªëng ch·ªâ ƒë·ªãnh|Li·ªÅu d√πng|$)', re.IGNORECASE | re.DOTALL),
            'contraindications': re.compile(r'(Ch·ªëng ch·ªâ ƒë·ªãnh|Kh√¥ng d√πng)[:\.]\s*(.+?)(?:\n|Li·ªÅu d√πng|C√°ch d√πng|$)', re.IGNORECASE | re.DOTALL),
            'dosage': re.compile(r'(Li·ªÅu d√πng|C√°ch d√πng|Li·ªÅu l∆∞·ª£ng)[:\.]\s*(.+?)(?:\n|T√°c d·ª•ng ph·ª•|$)', re.IGNORECASE | re.DOTALL)
        }
        
        # T√¨m t·ª´ng lo·∫°i th√¥ng tin
        for key, pattern in patterns.items():
            match = pattern.search(text)
            if match:
                details[key] = match.group(2).strip()[:500]  # Gi·ªõi h·∫°n ƒë·ªô d√†i
        
        return details
        
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói ƒë·ªçc PDF trang {page_number}: {e}")
        return {}

@app.route('/api/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        'status': 'ok',
        'message': 'Backend API is running',
        'drugs_loaded': len(drug_db) if drug_db is not None else 0
    })

@app.route('/api/scan', methods=['POST'])
def scan_drug():
    """API endpoint ƒë·ªÉ scan thu·ªëc t·ª´ ·∫£nh ho·∫∑c text"""
    try:
        # Ki·ªÉm tra xem c√≥ text ƒë∆∞·ª£c g·ª≠i tr·ª±c ti·∫øp kh√¥ng (t·ª´ modal x√°c nh·∫≠n OCR)
        if 'text' in request.json:
            confirmed_text = request.json['text']
            print(f"üìù T√¨m ki·∫øm v·ªõi text ƒë√£ x√°c nh·∫≠n: {confirmed_text}")
            
            # T√¨m ki·∫øm trong database
            drug_info = search_drug_in_database(confirmed_text)
            
            if drug_info:
                # Ki·ªÉm tra thu·ªëc k√™ ƒë∆°n
                is_prescription = drug_info.get('Is_Prescription', False)
                if isinstance(is_prescription, str):
                    is_prescription = is_prescription.lower() in ['true', '1', 'yes']
                elif pd.isna(is_prescription):
                    is_prescription = False
                
                if is_prescription:
                    return jsonify({
                        'success': False,
                        'error': 'PRESCRIPTION_REQUIRED',
                        'message': '‚ö†Ô∏è ƒê√¢y l√† thu·ªëc k√™ ƒë∆°n. Vui l√≤ng s·ª≠ d·ª•ng theo ch·ªâ ƒë·ªãnh c·ªßa b√°c sƒ©.',
                        'drug_name': drug_info.get('DrugName', ''),
                        'active_ingredient': drug_info.get('ActiveIngredient', ''),
                        'category': drug_info.get('Category', ''),
                        'extracted_text': confirmed_text
                    }), 403
                
                # L·∫•y th√¥ng tin t·ª´ PDF
                page_number = drug_info.get('PageNumber', '')
                pdf_details = {}
                if page_number and pdf_reader:
                    pdf_details = extract_drug_details_from_pdf(page_number)
                
                return jsonify({
                    'success': True,
                    'drug_name': drug_info.get('DrugName', ''),
                    'active_ingredient': drug_info.get('ActiveIngredient', ''),
                    'page_number': str(page_number),
                    'category': drug_info.get('Category', ''),
                    'extracted_text': confirmed_text,
                    'rx_status': 'OTC',
                    'composition': pdf_details.get('composition', ''),
                    'indications': pdf_details.get('indications', ''),
                    'contraindications': pdf_details.get('contraindications', ''),
                    'dosage': pdf_details.get('dosage', '')
                })
            else:
                return jsonify({
                    'success': False,
                    'message': 'Kh√¥ng t√¨m th·∫•y th√¥ng tin thu·ªëc trong database',
                    'extracted_text': confirmed_text
                }), 404
        
        # Ki·ªÉm tra xem c√≥ file ho·∫∑c base64 image kh√¥ng
        if 'image' in request.files:
            # Nh·∫≠n file upload
            file = request.files['image']
            if file and allowed_file(file.filename):
                # ƒê·ªçc ·∫£nh t·ª´ file
                image_bytes = file.read()
                image = Image.open(io.BytesIO(image_bytes))
                if image.mode != 'RGB':
                    image = image.convert('RGB')
                image_array = np.array(image)
        elif 'image' in request.json:
            # Nh·∫≠n base64 image
            base64_image = request.json['image']
            image_array = decode_base64_image(base64_image)
            if image_array is None:
                return jsonify({'error': 'Invalid image data'}), 400
        else:
            return jsonify({'error': 'No image provided'}), 400
        
        # Ti·ªÅn x·ª≠ l√Ω ·∫£nh
        processed_image = preprocess_image(image_array)
        
        # Tr√≠ch xu·∫•t text t·ª´ ·∫£nh (OCR) - tr·∫£ v·ªÅ text ƒë√£ ch·ªçn v√† t·∫•t c·∫£ text
        extracted_text, all_ocr_texts = extract_text_from_image(image_array)  # D√πng ·∫£nh g·ªëc
        
        if not extracted_text:
            return jsonify({
                'success': False,
                'message': 'Kh√¥ng th·ªÉ nh·∫≠n di·ªán text t·ª´ ·∫£nh. Vui l√≤ng th·ª≠ l·∫°i v·ªõi ·∫£nh r√µ h∆°n.',
                'extracted_text': '',
                'all_ocr_texts': all_ocr_texts
            }), 400
        
        print(f"üìù Text nh·∫≠n di·ªán ƒë∆∞·ª£c: {extracted_text}")
        print(f"üìã T·∫•t c·∫£ text OCR: {all_ocr_texts}")
        
        # T√¨m ki·∫øm trong database
        drug_info = search_drug_in_database(extracted_text)
        
        if drug_info:
            # KI·ªÇM TRA AN TO√ÄN: N·∫øu l√† thu·ªëc k√™ ƒë∆°n (Is_Prescription = True), ch·∫∑n l·∫°i
            is_prescription = drug_info.get('Is_Prescription', False)
            
            # Chuy·ªÉn ƒë·ªïi gi√° tr·ªã boolean t·ª´ CSV (c√≥ th·ªÉ l√† string "True"/"False" ho·∫∑c boolean)
            if isinstance(is_prescription, str):
                is_prescription = is_prescription.lower() in ['true', '1', 'yes']
            elif pd.isna(is_prescription):
                is_prescription = False
            
            if is_prescription:
                return jsonify({
                    'success': False,
                    'error': 'PRESCRIPTION_REQUIRED',
                    'message': '‚ö†Ô∏è ƒê√¢y l√† thu·ªëc k√™ ƒë∆°n. Vui l√≤ng s·ª≠ d·ª•ng theo ch·ªâ ƒë·ªãnh c·ªßa b√°c sƒ©.',
                    'drug_name': drug_info.get('DrugName', ''),
                    'active_ingredient': drug_info.get('ActiveIngredient', ''),
                    'category': drug_info.get('Category', ''),
                    'extracted_text': extracted_text,
                    'all_ocr_texts': all_ocr_texts  # Tr·∫£ v·ªÅ t·∫•t c·∫£ text OCR
                }), 403  # 403 Forbidden
            
            # N·∫øu l√† thu·ªëc OTC, ti·∫øp t·ª•c tra c·ª©u th√¥ng tin chi ti·∫øt t·ª´ PDF
            page_number = drug_info.get('PageNumber', '')
            pdf_details = {}
            
            if page_number and pdf_reader:
                pdf_details = extract_drug_details_from_pdf(page_number)
            
            # Tr·∫£ v·ªÅ th√¥ng tin thu·ªëc ƒë·∫ßy ƒë·ªß
            return jsonify({
                'success': True,
                'drug_name': drug_info.get('DrugName', ''),
                'active_ingredient': drug_info.get('ActiveIngredient', ''),
                'page_number': str(page_number),
                'category': drug_info.get('Category', ''),
                'extracted_text': extracted_text,
                'all_ocr_texts': all_ocr_texts,  # Tr·∫£ v·ªÅ t·∫•t c·∫£ text OCR
                'rx_status': 'OTC',
                'composition': pdf_details.get('composition', ''),
                'indications': pdf_details.get('indications', ''),
                'contraindications': pdf_details.get('contraindications', ''),
                'dosage': pdf_details.get('dosage', '')
            })
        else:
            # Kh√¥ng t√¨m th·∫•y trong database - tr·∫£ v·ªÅ ƒë·ªÉ user c√≥ th·ªÉ x√°c nh·∫≠n OCR
            return jsonify({
                'success': False,
                'needs_ocr_confirm': True,  # Flag ƒë·ªÉ frontend hi·ªÉn th·ªã modal x√°c nh·∫≠n
                'message': 'Kh√¥ng t√¨m th·∫•y th√¥ng tin thu·ªëc trong database',
                'extracted_text': extracted_text,
                'all_ocr_texts': all_ocr_texts  # Tr·∫£ v·ªÅ t·∫•t c·∫£ text OCR
            }), 404
            
    except Exception as e:
        print(f"Error processing scan: {e}")
        return jsonify({
            'error': 'Internal server error',
            'message': str(e)
        }), 500

@app.route('/api/drugs/search', methods=['GET'])
def search_drugs():
    """API endpoint ƒë·ªÉ t√¨m ki·∫øm thu·ªëc theo t√™n"""
    query = request.args.get('q', '')
    if not query:
        return jsonify({'error': 'Query parameter required'}), 400
    
    if drug_db is None or drug_db.empty:
        return jsonify({'drugs': []})
    
    # T√¨m ki·∫øm
    query_lower = query.lower()
    results = drug_db[drug_db['DrugName'].str.lower().str.contains(query_lower, na=False)]
    
    # Gi·ªõi h·∫°n k·∫øt qu·∫£
    results = results.head(20)
    
    return jsonify({
        'drugs': results.to_dict('records')
    })

if __name__ == '__main__':
    load_drug_database()
    load_pdf()
    print("üöÄ Starting MediScan AI Backend Server...")
    print("üì° API available at http://localhost:5000")
    app.run(debug=True, host='0.0.0.0', port=5000)

