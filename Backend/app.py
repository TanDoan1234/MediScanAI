from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import base64
import cv2
import numpy as np
from PIL import Image
import io
import pandas as pd
import re
from werkzeug.utils import secure_filename
from pypdf import PdfReader
import google.generativeai as genai

# Load environment variables from .env file
try:
    from dotenv import load_dotenv
    # Load .env file t·ª´ th∆∞ m·ª•c Backend
    env_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '.env')
    load_dotenv(env_path)
    print(f"‚úÖ ƒê√£ load .env t·ª´: {env_path}")
except ImportError:
    print("‚ö†Ô∏è python-dotenv ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. S·ª≠ d·ª•ng environment variables t·ª´ h·ªá th·ªëng.")
except Exception as e:
    print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ load .env file: {e}")

# Fix cho Pillow 10.0+ kh√¥ng c√≤n Image.ANTIALIAS
# EasyOCR v√† m·ªôt s·ªë th∆∞ vi·ªán v·∫´n c·∫ßn ANTIALIAS
if not hasattr(Image, 'ANTIALIAS'):
    Image.ANTIALIAS = Image.LANCZOS

app = Flask(__name__)
# C·∫•u h√¨nh CORS chi ti·∫øt ƒë·ªÉ h·ªó tr·ª£ port forwarding
CORS(app, 
     resources={r"/api/*": {
         "origins": "*",
         "methods": ["GET", "POST", "OPTIONS"],
         "allow_headers": ["Content-Type", "Authorization"]
     }},
     supports_credentials=True)

# C·∫•u h√¨nh
UPLOAD_FOLDER = 'uploads'
ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif', 'webp'}

if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size

# Load drug database v√† PDF
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DRUG_DB_PATH = os.path.join(BASE_DIR, '..', 'Crawldata', 'drug_database_refined.csv')
PDF_PATH = os.path.join(BASE_DIR, '..', 'Crawldata', 'duoc-thu-quoc-gia-viet-nam-2018.pdf')
drug_db = None
pdf_reader = None
ocr_reader = None  # EasyOCR reader (cache ƒë·ªÉ kh√¥ng load l·∫°i m·ªói l·∫ßn)

def load_drug_database():
    """Load drug database t·ª´ CSV file"""
    global drug_db
    try:
        if os.path.exists(DRUG_DB_PATH):
            drug_db = pd.read_csv(DRUG_DB_PATH)
            print(f"‚úÖ ƒê√£ load {len(drug_db)} thu·ªëc t·ª´ database")
        else:
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file database t·∫°i: {DRUG_DB_PATH}")
            drug_db = pd.DataFrame()
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ load database: {e}")
        drug_db = pd.DataFrame()

def load_pdf():
    """Load PDF d∆∞·ª£c th∆∞ qu·ªëc gia"""
    global pdf_reader
    try:
        if os.path.exists(PDF_PATH):
            pdf_reader = PdfReader(PDF_PATH)
            print(f"‚úÖ ƒê√£ load PDF v·ªõi {len(pdf_reader.pages)} trang")
        else:
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file PDF t·∫°i: {PDF_PATH}")
            pdf_reader = None
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ load PDF: {e}")
        pdf_reader = None

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def decode_base64_image(base64_string):
    """Decode base64 string th√†nh image"""
    try:
        if not base64_string:
            print("‚ùå Base64 string r·ªóng")
            return None
        
        # Remove data URL prefix if present (data:image/jpeg;base64,...)
        if ',' in base64_string:
            base64_string = base64_string.split(',')[1]
        
        # Lo·∫°i b·ªè whitespace
        base64_string = base64_string.strip()
        
        # Decode base64 v·ªõi validation
        try:
            image_data = base64.b64decode(base64_string, validate=True)
        except Exception as e:
            print(f"‚ùå L·ªói decode base64: {e}")
            return None
        
        if len(image_data) == 0:
            print("‚ùå Image bytes r·ªóng sau khi decode")
            return None
        
        # M·ªü ·∫£nh b·∫±ng PIL
        try:
            image = Image.open(io.BytesIO(image_data))
        except Exception as e:
            print(f"‚ùå L·ªói m·ªü ·∫£nh t·ª´ bytes: {e}")
            return None
        
        # Convert to RGB if necessary
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        image_array = np.array(image)
        if image_array.size == 0:
            print("‚ùå Image array r·ªóng")
            return None
        
        print(f"‚úÖ Decode ·∫£nh th√†nh c√¥ng: {image_array.shape}")
        return image_array
    except Exception as e:
        print(f"‚ùå Error decoding image: {e}")
        import traceback
        print(traceback.format_exc())
        return None

def preprocess_image(image_array):
    """Ti·ªÅn x·ª≠ l√Ω ·∫£nh ƒë·ªÉ c·∫£i thi·ªán OCR"""
    # Convert to grayscale
    if len(image_array.shape) == 3:
        gray = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)
    else:
        gray = image_array
    
    # Apply denoising
    denoised = cv2.fastNlMeansDenoising(gray, None, 10, 7, 21)
    
    # Apply threshold
    _, thresh = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    
    # Resize n·∫øu ·∫£nh qu√° nh·ªè (t·ªëi thi·ªÉu 300px width ƒë·ªÉ OCR t·ªët h∆°n)
    height, width = thresh.shape
    if width < 300:
        scale = 300 / width
        new_width = int(width * scale)
        new_height = int(height * scale)
        thresh = cv2.resize(thresh, (new_width, new_height), interpolation=cv2.INTER_CUBIC)
    
    return thresh

def get_ocr_reader():
    """L·∫•y ho·∫∑c kh·ªüi t·∫°o EasyOCR reader (cache ƒë·ªÉ kh√¥ng load l·∫°i m·ªói l·∫ßn)"""
    global ocr_reader
    if ocr_reader is None:
        try:
            import easyocr
            print("üîÑ ƒêang kh·ªüi t·∫°o EasyOCR (l·∫ßn ƒë·∫ßu c√≥ th·ªÉ m·∫•t v√†i ph√∫t ƒë·ªÉ t·∫£i model)...")
            # H·ªó tr·ª£ ti·∫øng Vi·ªát v√† ti·∫øng Anh
            ocr_reader = easyocr.Reader(['vi', 'en'], gpu=False)
            print("‚úÖ EasyOCR ƒë√£ s·∫µn s√†ng!")
        except ImportError:
            print("‚ö†Ô∏è EasyOCR ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Ch·∫°y: pip install easyocr")
            return None
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói kh·ªüi t·∫°o EasyOCR: {e}")
            return None
    return ocr_reader

def extract_text_from_image(image_array):
    """Tr√≠ch xu·∫•t text t·ª´ ·∫£nh s·ª≠ d·ª•ng EasyOCR, tr·∫£ v·ªÅ t·∫•t c·∫£ text v√† text ƒë∆∞·ª£c ch·ªçn"""
    try:
        reader = get_ocr_reader()
        if reader is None:
            print("‚ùå OCR reader is None")
            return None, []
        
        # EasyOCR c·∫ßn ·∫£nh ·ªü d·∫°ng numpy array (BGR ho·∫∑c RGB)
        if len(image_array.shape) == 2:
            image_rgb = cv2.cvtColor(image_array, cv2.COLOR_GRAY2RGB)
        else:
            image_rgb = image_array
        
        # Chuy·ªÉn t·ª´ RGB sang BGR (OpenCV format)
        image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)
        height, width = image_bgr.shape[:2]
        print(f"üìê ·∫¢nh k√≠ch th∆∞·ªõc: {width}x{height}")
        
        # OCR v·ªõi EasyOCR
        print("üîç ƒêang ch·∫°y OCR...")
        results = reader.readtext(image_bgr)
        print(f"üìä OCR t√¨m th·∫•y {len(results) if results else 0} text regions")
        
        if not results:
            print("‚ö†Ô∏è OCR kh√¥ng t√¨m th·∫•y text n√†o trong ·∫£nh")
            return None, []
        
        # Danh s√°ch t·ª´ th√¥ng th∆∞·ªùng c·∫ßn lo·∫°i b·ªè (kh√¥ng ph·∫£i t√™n thu·ªëc)
        common_words = {
            'arthritis', 'pain', 'relief', 'fever', 'reducer', 'temporary', 'minor',
            'tablets', 'caplets', 'mg', 'each', 'extended', 'release', 'acetaminophen',
            'ibuprofen', 'aspirin', 'do', 'not', 'use', 'with', 'other', 'medicines',
            'containing', 'to', 'open', 'push', 'turn', 'cap', 'warnings', 'directions',
            'store', 'at', 'room', 'temperature', 'keep', 'out', 'of', 'reach', 'children',
            'active', 'ingredient', 'inactive', 'ingredients', 'see', 'package', 'insert'
        }
        
        # L·∫•y t·∫•t c·∫£ text v·ªõi th√¥ng tin chi ti·∫øt
        all_texts = []
        candidate_texts = []
        
        for (bbox, text, confidence) in results:
            print(f"  üìù Text: '{text}' - Confidence: {confidence:.2f}")
            if confidence > 0.2:  # Gi·∫£m ng∆∞·ª°ng xu·ªëng 20% ƒë·ªÉ b·∫Øt ƒë∆∞·ª£c nhi·ªÅu text h∆°n
                # L√†m s·∫°ch text: lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát ·ªü ƒë·∫ßu/cu·ªëi
                text_cleaned = text.strip()
                # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát th∆∞·ªùng g·∫∑p trong OCR
                text_cleaned = text_cleaned.strip('[](){}.,;:!?-_=+')
                text_clean = text_cleaned.upper()
                
                # B·ªè qua n·∫øu text qu√° ng·∫Øn sau khi l√†m s·∫°ch
                if len(text_cleaned) < 2:
                    continue
                
                # T√≠nh to√°n v·ªã tr√≠ trung t√¢m c·ªßa text
                x_coords = [point[0] for point in bbox]
                y_coords = [point[1] for point in bbox]
                center_x = sum(x_coords) / len(x_coords)
                center_y = sum(y_coords) / len(y_coords)
                
                # T√≠nh kho·∫£ng c√°ch t·ª´ trung t√¢m ·∫£nh (∆∞u ti√™n text ·ªü gi·ªØa)
                distance_from_center = ((center_x - width/2)**2 + (center_y - height/2)**2)**0.5
                normalized_distance = distance_from_center / ((width/2)**2 + (height/2)**2)**0.5
                
                # T√≠nh k√≠ch th∆∞·ªõc text (∆∞u ti√™n text l·ªõn)
                text_width = max(x_coords) - min(x_coords)
                text_height = max(y_coords) - min(y_coords)
                text_area = text_width * text_height
                normalized_area = text_area / (width * height)
                
                # Lo·∫°i b·ªè t·ª´ th√¥ng th∆∞·ªùng
                words = text_clean.split()
                is_common = all(word in common_words for word in words if len(word) > 2)
                
                all_texts.append({
                    'text': text_cleaned,  # D√πng text ƒë√£ l√†m s·∫°ch
                    'confidence': confidence,
                    'center_x': center_x,
                    'center_y': center_y,
                    'distance_from_center': normalized_distance,
                    'area': normalized_area
                })
                
                # Ch·ªâ th√™m v√†o candidate n·∫øu kh√¥ng ph·∫£i t·ª´ th√¥ng th∆∞·ªùng
                if not is_common and len(text_clean) >= 3:
                    # T√≠nh ƒëi·ªÉm ∆∞u ti√™n: confidence cao, ·ªü gi·ªØa, k√≠ch th∆∞·ªõc l·ªõn, kh√¥ng ph·∫£i s·ªë thu·∫ßn t√∫y
                    is_number_only = text_clean.replace(' ', '').replace('.', '').isdigit()
                    if not is_number_only:
                        score = (
                            confidence * 0.4 +  # 40% t·ª´ confidence
                            (1 - normalized_distance) * 0.3 +  # 30% t·ª´ v·ªã tr√≠ (g·∫ßn trung t√¢m h∆°n = t·ªët h∆°n)
                            normalized_area * 0.2 +  # 20% t·ª´ k√≠ch th∆∞·ªõc
                            min(len(text_clean) / 20, 1) * 0.1  # 10% t·ª´ ƒë·ªô d√†i (∆∞u ti√™n text v·ª´a ph·∫£i)
                        )
                        candidate_texts.append({
                            'text': text,
                            'score': score,
                            'confidence': confidence
                        })
        
        if not all_texts:
            print("‚ö†Ô∏è Kh√¥ng c√≥ text n√†o sau khi filter (confidence > 0.2)")
            # Fallback: L·∫•y t·∫•t c·∫£ text d√π confidence th·∫•p
            print("üîÑ Th·ª≠ l·∫•y t·∫•t c·∫£ text (kh√¥ng filter confidence)...")
            for (bbox, text, confidence) in results:
                text_cleaned = text.strip().strip('[](){}.,;:!?-_=+')
                if len(text_cleaned) >= 2:
                    x_coords = [point[0] for point in bbox]
                    y_coords = [point[1] for point in bbox]
                    center_x = sum(x_coords) / len(x_coords)
                    center_y = sum(y_coords) / len(y_coords)
                    text_width = max(x_coords) - min(x_coords)
                    text_height = max(y_coords) - min(y_coords)
                    text_area = text_width * text_height
                    normalized_area = text_area / (width * height)
                    
                    all_texts.append({
                        'text': text_cleaned,
                        'confidence': confidence,
                        'center_x': center_x,
                        'center_y': center_y,
                        'distance_from_center': ((center_x - width/2)**2 + (center_y - height/2)**2)**0.5 / ((width/2)**2 + (height/2)**2)**0.5,
                        'area': normalized_area
                    })
            
            if not all_texts:
                print("‚ùå V·∫´n kh√¥ng c√≥ text n√†o sau khi l·∫•y t·∫•t c·∫£")
                return None, []
            else:
                print(f"‚úÖ ƒê√£ l·∫•y ƒë∆∞·ª£c {len(all_texts)} text (kh√¥ng filter confidence)")
        
        # S·∫Øp x·∫øp candidate theo ƒëi·ªÉm s·ªë
        candidate_texts.sort(key=lambda x: x['score'], reverse=True)
        
        # C·∫£i thi·ªán: K·∫øt h·ª£p c√°c text g·∫ßn nhau th√†nh t√™n thu·ªëc ƒë·∫ßy ƒë·ªß
        # T√¨m c√°c text c√≥ th·ªÉ l√† m·ªôt ph·∫ßn c·ªßa t√™n thu·ªëc (c√≥ s·ªë %, ch·ªØ hoa, v.v.)
        combined_candidates = []
        
        # ∆Øu ti√™n text c√≥ ch·ª©a s·ªë ph·∫ßn trƒÉm (%)
        percent_texts = [t for t in all_texts if '%' in t['text'] or '10' in t['text'] or '20' in t['text']]
        
        # T√¨m text c√≥ v·∫ª l√† t√™n thu·ªëc (ch·ªØ hoa, kh√¥ng ph·∫£i t·ª´ th√¥ng th∆∞·ªùng)
        drug_name_candidates = []
        for t in all_texts:
            text_upper = t['text'].upper()
            # Lo·∫°i b·ªè text ch·ªâ l√† s·ªë ho·∫∑c qu√° ng·∫Øn
            if len(text_upper) >= 3 and not text_upper.replace(' ', '').replace('.', '').replace('%', '').isdigit():
                # Ki·ªÉm tra xem c√≥ ph·∫£i t·ª´ th√¥ng th∆∞·ªùng kh√¥ng
                words = text_upper.split()
                is_common = any(word in common_words for word in words if len(word) > 2)
                if not is_common:
                    drug_name_candidates.append(t)
        
        # K·∫øt h·ª£p text: ∆Øu ti√™n text c√≥ %, sau ƒë√≥ k·∫øt h·ª£p v·ªõi text kh√°c g·∫ßn nhau
        if percent_texts and drug_name_candidates:
            # T√¨m text g·∫ßn v·ªõi text c√≥ %
            for percent_text in percent_texts:
                percent_center = (percent_text['center_x'], percent_text['center_y'])
                # T√¨m text g·∫ßn nh·∫•t (trong v√≤ng 200px)
                nearby_texts = []
                for candidate in drug_name_candidates:
                    distance = ((candidate['center_x'] - percent_center[0])**2 + 
                               (candidate['center_y'] - percent_center[1])**2)**0.5
                    if distance < 200:  # Text trong v√≤ng 200px
                        nearby_texts.append((candidate, distance))
                
                # S·∫Øp x·∫øp theo kho·∫£ng c√°ch
                nearby_texts.sort(key=lambda x: x[1])
                
                # K·∫øt h·ª£p text g·∫ßn nhau
                if nearby_texts:
                    combined = [percent_text['text']]
                    for candidate, _ in nearby_texts[:2]:  # L·∫•y t·ªëi ƒëa 2 text g·∫ßn nh·∫•t
                        if candidate['text'] not in combined:
                            combined.append(candidate['text'])
                    combined_text = ' '.join(combined)
                    combined_candidates.append({
                        'text': combined_text,
                        'score': 0.9,  # ƒêi·ªÉm cao cho text k·∫øt h·ª£p
                        'confidence': min(percent_text['confidence'], nearby_texts[0][0]['confidence'])
                    })
        
        # N·∫øu c√≥ text k·∫øt h·ª£p, ∆∞u ti√™n n√≥
        if combined_candidates:
            combined_candidates.sort(key=lambda x: x['score'], reverse=True)
            selected_text = combined_candidates[0]['text']
        elif candidate_texts:
            selected_text = candidate_texts[0]['text']
        else:
            # N·∫øu kh√¥ng c√≥ candidate, ch·ªçn text d√†i nh·∫•t kh√¥ng ph·∫£i t·ª´ th√¥ng th∆∞·ªùng
            filtered = [t for t in all_texts if not any(w in common_words for w in t['text'].upper().split())]
            if filtered:
                selected_text = max(filtered, key=lambda x: len(x['text']))['text']
            else:
                selected_text = max(all_texts, key=lambda x: len(x['text']))['text']
        
        # Tr·∫£ v·ªÅ text ƒë√£ ch·ªçn v√† danh s√°ch t·∫•t c·∫£ text
        all_texts_list = [t['text'] for t in all_texts]
        return selected_text, all_texts_list
        
    except Exception as e:
        import traceback
        print(f"‚ùå L·ªói OCR: {e}")
        print(f"üìã Traceback:\n{traceback.format_exc()}")
        return None, []

def search_drug_in_database(drug_name, all_ocr_texts=None):
    """T√¨m ki·∫øm thu·ªëc trong database - c·∫£i thi·ªán v·ªõi fuzzy matching v√† t√¨m theo ho·∫°t ch·∫•t"""
    if drug_db is None or drug_db.empty:
        return None
    
    # L√†m s·∫°ch text: lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát c√≥ th·ªÉ g√¢y l·ªói regex
    drug_name_clean = drug_name.strip()
    # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát ·ªü ƒë·∫ßu/cu·ªëi nh∆∞ [ ] ( ) { }
    drug_name_clean = drug_name_clean.strip('[](){}')
    drug_name_lower = drug_name_clean.lower().strip()
    
    if not drug_name_lower:
        return None
    
    print(f"üîç T√¨m ki·∫øm thu·ªëc: '{drug_name_clean}'")
    
    # T√¨m exact match trong DrugName
    exact_match = drug_db[drug_db['DrugName'].str.lower() == drug_name_lower]
    if not exact_match.empty:
        print(f"‚úÖ T√¨m th·∫•y exact match: {exact_match.iloc[0]['DrugName']}")
        return exact_match.iloc[0].to_dict()
    
    # T√¨m partial match trong DrugName
    try:
        partial_match = drug_db[drug_db['DrugName'].str.lower().str.contains(drug_name_lower, na=False, regex=False)]
        if not partial_match.empty:
            print(f"‚úÖ T√¨m th·∫•y partial match: {partial_match.iloc[0]['DrugName']}")
            return partial_match.iloc[0].to_dict()
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói t√¨m ki·∫øm partial match: {e}")
    
    # T√¨m theo t·ª´ kh√≥a trong DrugName
    keywords = drug_name_lower.split()
    for keyword in keywords:
        if len(keyword) > 3:
            keyword_clean = keyword.strip('[](){}.,;:!?')
            if len(keyword_clean) > 3:
                try:
                    keyword_match = drug_db[drug_db['DrugName'].str.lower().str.contains(keyword_clean, na=False, regex=False)]
                    if not keyword_match.empty:
                        print(f"‚úÖ T√¨m th·∫•y theo keyword '{keyword_clean}': {keyword_match.iloc[0]['DrugName']}")
                        return keyword_match.iloc[0].to_dict()
                except Exception as e:
                    print(f"‚ö†Ô∏è L·ªói t√¨m ki·∫øm keyword '{keyword_clean}': {e}")
                    continue
    
    # N·∫øu kh√¥ng t√¨m th·∫•y, th·ª≠ t√¨m trong ActiveIngredient
    print(f"üîç Kh√¥ng t√¨m th·∫•y trong DrugName, th·ª≠ t√¨m trong ActiveIngredient...")
    try:
        ingredient_match = drug_db[drug_db['ActiveIngredient'].str.lower().str.contains(drug_name_lower, na=False, regex=False)]
        if not ingredient_match.empty:
            print(f"‚úÖ T√¨m th·∫•y theo ho·∫°t ch·∫•t: {ingredient_match.iloc[0]['DrugName']} ({ingredient_match.iloc[0]['ActiveIngredient']})")
            return ingredient_match.iloc[0].to_dict()
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói t√¨m ki·∫øm trong ActiveIngredient: {e}")
    
    # N·∫øu c√≥ all_ocr_texts, th·ª≠ t√¨m v·ªõi c√°c text kh√°c c√≥ confidence cao
    if all_ocr_texts:
        print(f"üîç Th·ª≠ t√¨m v·ªõi c√°c text OCR kh√°c: {all_ocr_texts[:5]}")
        for ocr_text in all_ocr_texts[:5]:  # Th·ª≠ 5 text ƒë·∫ßu ti√™n
            if ocr_text and len(ocr_text.strip()) > 3:
                ocr_clean = ocr_text.strip().lower()
                try:
                    # T√¨m trong DrugName
                    ocr_match = drug_db[drug_db['DrugName'].str.lower().str.contains(ocr_clean, na=False, regex=False)]
                    if not ocr_match.empty:
                        print(f"‚úÖ T√¨m th·∫•y v·ªõi text OCR '{ocr_text}': {ocr_match.iloc[0]['DrugName']}")
                        return ocr_match.iloc[0].to_dict()
                    
                    # T√¨m trong ActiveIngredient
                    ocr_ingredient_match = drug_db[drug_db['ActiveIngredient'].str.lower().str.contains(ocr_clean, na=False, regex=False)]
                    if not ocr_ingredient_match.empty:
                        print(f"‚úÖ T√¨m th·∫•y ho·∫°t ch·∫•t v·ªõi text OCR '{ocr_text}': {ocr_ingredient_match.iloc[0]['DrugName']} ({ocr_ingredient_match.iloc[0]['ActiveIngredient']})")
                        return ocr_ingredient_match.iloc[0].to_dict()
                except:
                    continue
    
    print(f"‚ùå Kh√¥ng t√¨m th·∫•y thu·ªëc: '{drug_name_clean}'")
    return None

def summarize_drug_info_with_gemini(pdf_text, drug_name, drug_info):
    """
    S·ª≠ d·ª•ng Gemini AI ƒë·ªÉ ƒë·ªçc to√†n b·ªô th√¥ng tin t·ª´ PDF v√† t·ªïng h·ª£p th√†nh:
    - C√°ch d√πng (usage): D·ªÖ hi·ªÉu, ng·∫Øn g·ªçn
    - L∆∞u √Ω (notes): T·ª´ ch·ªëng ch·ªâ ƒë·ªãnh, t∆∞∆°ng t√°c thu·ªëc, t√°c d·ª•ng ph·ª•
    """
    if not pdf_text or len(pdf_text.strip()) < 50:
        print("‚ö†Ô∏è PDF text qu√° ng·∫Øn ho·∫∑c r·ªóng, kh√¥ng th·ªÉ t·ªïng h·ª£p")
        return {
            'usage': 'Th√¥ng tin c√°ch d√πng kh√¥ng c√≥ trong d∆∞·ª£c th∆∞ cho thu·ªëc n√†y.',
            'notes': 'Th√¥ng tin l∆∞u √Ω kh√¥ng c√≥ trong d∆∞·ª£c th∆∞ cho thu·ªëc n√†y.'
        }
    
    # L·∫•y API key t·ª´ environment variable
    gemini_api_key = os.getenv('GEMINI_API_KEY')
    if not gemini_api_key:
        print("‚ö†Ô∏è GEMINI_API_KEY kh√¥ng ƒë∆∞·ª£c c·∫•u h√¨nh, tr·∫£ v·ªÅ 'kh√¥ng c√≥'")
        return {
            'usage': 'Th√¥ng tin c√°ch d√πng kh√¥ng c√≥ trong d∆∞·ª£c th∆∞ cho thu·ªëc n√†y.',
            'notes': 'Th√¥ng tin l∆∞u √Ω kh√¥ng c√≥ trong d∆∞·ª£c th∆∞ cho thu·ªëc n√†y.'
        }
    
    try:
        # C·∫•u h√¨nh Gemini
        genai.configure(api_key=gemini_api_key)
        # S·ª≠ d·ª•ng Gemini 2.0 Flash (model m·ªõi nh·∫•t, nhanh v√† ch√≠nh x√°c)
        try:
            model = genai.GenerativeModel('gemini-2.0-flash-exp')
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ d√πng gemini-2.0-flash-exp, th·ª≠ gemini-2.0-flash: {e}")
            try:
                model = genai.GenerativeModel('gemini-2.0-flash')
            except Exception as e2:
                print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ d√πng gemini-2.0-flash, th·ª≠ gemini-1.5-flash: {e2}")
                model = genai.GenerativeModel('gemini-1.5-flash')
        
        # L·∫•y th√¥ng tin b·ªï sung t·ª´ drug_info
        category = drug_info.get('Category', '')
        active_ingredient = drug_info.get('ActiveIngredient', '')
        
        # Gi·ªõi h·∫°n ƒë·ªô d√†i PDF text ƒë·ªÉ tr√°nh v∆∞·ª£t qu√° token limit
        pdf_text_limited = pdf_text[:3000] if len(pdf_text) > 3000 else pdf_text
        
        # Prompt ƒë·ªÉ t·ªïng h·ª£p th√¥ng tin - c·∫£i thi·ªán ƒë·ªÉ filter ƒë√∫ng thu·ªëc v√† kh√¥ng b·ªãa ra th√¥ng tin
        prompt = f"""B·∫°n l√† m·ªôt d∆∞·ª£c sƒ© chuy√™n nghi·ªáp. H√£y ƒë·ªçc v√† t·ªïng h·ª£p th√¥ng tin t·ª´ D∆∞·ª£c th∆∞ Qu·ªëc gia v·ªÅ thu·ªëc C·ª§ TH·ªÇ sau:

**THU·ªêC C·∫¶N T√åM:**
- T√™n thu·ªëc: {drug_name}
- Ho·∫°t ch·∫•t: {active_ingredient}
- Ph√¢n lo·∫°i: {category}

**L∆ØU √ù QUAN TR·ªåNG - ƒê·ªåC K·ª∏:**
- Trang PDF c√≥ th·ªÉ ch·ª©a th√¥ng tin c·ªßa NHI·ªÄU thu·ªëc kh√°c nhau
- B·∫†N CH·ªà ƒê∆Ø·ª¢C t·ªïng h·ª£p th√¥ng tin v·ªÅ thu·ªëc "{drug_name}" ho·∫∑c "{active_ingredient}"
- B·ªé QUA ho√†n to√†n th√¥ng tin v·ªÅ c√°c thu·ªëc kh√°c (nh∆∞ Polymyxin, Polygelin, ho·∫∑c b·∫•t k·ª≥ thu·ªëc n√†o kh√°c)
- **QUAN TR·ªåNG NH·∫§T: N·∫æU KH√îNG T√åM TH·∫§Y TH√îNG TIN V·ªÄ THU·ªêC N√ÄY TRONG PDF, B·∫†N PH·∫¢I TR·∫¢ V·ªÄ "KH√îNG C√ì TRONG D∆Ø·ª¢C TH∆Ø"**
- **TUY·ªÜT ƒê·ªêI KH√îNG ƒê∆Ø·ª¢C B·ªäA RA, T·∫†O RA, HO·∫∂C SUY ƒêO√ÅN TH√îNG TIN KH√îNG C√ì TRONG PDF**
- **CH·ªà T·ªîNG H·ª¢P TH√îNG TIN C√ì TH·∫¨T TRONG PDF, KH√îNG TH√äM B·∫§T K·ª≤ TH√îNG TIN N√ÄO KH√îNG C√ì TRONG PDF**

**Th√¥ng tin t·ª´ D∆∞·ª£c th∆∞ (c√≥ th·ªÉ ch·ª©a nhi·ªÅu thu·ªëc):**
{pdf_text_limited}

**Y√äU C·∫¶U:**
1. T·ªïng h·ª£p ph·∫ßn "C√ÅCH D√ôNG" (usage) - CH·ªà v·ªÅ thu·ªëc "{drug_name}":
   - **CH·ªà t·ªïng h·ª£p th√¥ng tin C√ì TH·∫¨T trong PDF v·ªÅ thu·ªëc n√†y**
   - Vi·∫øt b·∫±ng ng√¥n ng·ªØ ƒë∆°n gi·∫£n, d·ªÖ hi·ªÉu
   - T·∫≠p trung v√†o: li·ªÅu l∆∞·ª£ng, th·ªùi ƒëi·ªÉm u·ªëng, c√°ch u·ªëng, t·∫ßn su·∫•t
   - S·ª≠ d·ª•ng c√¢u ng·∫Øn g·ªçn, r√µ r√†ng
   - Lo·∫°i b·ªè thu·∫≠t ng·ªØ y khoa ph·ª©c t·∫°p
   - **N·∫æU KH√îNG T√åM TH·∫§Y TH√îNG TIN V·ªÄ THU·ªêC N√ÄY, B·∫†N PH·∫¢I VI·∫æT CH√çNH X√ÅC: "Th√¥ng tin c√°ch d√πng kh√¥ng c√≥ trong d∆∞·ª£c th∆∞ cho thu·ªëc n√†y."**
   - **KH√îNG ƒê∆Ø·ª¢C T·∫†O RA, B·ªäA RA, HO·∫∂C SUY ƒêO√ÅN TH√îNG TIN**

2. T·ªïng h·ª£p ph·∫ßn "L∆ØU √ù" (notes) - CH·ªà v·ªÅ thu·ªëc "{drug_name}":
   - **CH·ªà t·ªïng h·ª£p th√¥ng tin C√ì TH·∫¨T trong PDF v·ªÅ thu·ªëc n√†y**
   - T·ª´ ch·ªëng ch·ªâ ƒë·ªãnh: ai kh√¥ng n√™n d√πng
   - T∆∞∆°ng t√°c thu·ªëc: kh√¥ng d√πng c√πng v·ªõi thu·ªëc g√¨
   - T√°c d·ª•ng ph·ª•: c·∫ßn ch√∫ √Ω g√¨
   - ƒê·ªëi t∆∞·ª£ng ƒë·∫∑c bi·ªát: ph·ª• n·ªØ c√≥ thai, tr·∫ª em, ng∆∞·ªùi gi√†
   - B·∫£o qu·∫£n: c√°ch b·∫£o qu·∫£n thu·ªëc
   - **N·∫æU KH√îNG T√åM TH·∫§Y TH√îNG TIN V·ªÄ THU·ªêC N√ÄY, B·∫†N PH·∫¢I VI·∫æT CH√çNH X√ÅC: "Th√¥ng tin l∆∞u √Ω kh√¥ng c√≥ trong d∆∞·ª£c th∆∞ cho thu·ªëc n√†y."**
   - **KH√îNG ƒê∆Ø·ª¢C T·∫†O RA, B·ªäA RA, HO·∫∂C SUY ƒêO√ÅN TH√îNG TIN**

**Tr·∫£ v·ªÅ theo ƒë·ªãnh d·∫°ng JSON:**
{{
  "usage": "Ph·∫ßn c√°ch d√πng (CH·ªà th√¥ng tin c√≥ th·∫≠t trong PDF v·ªÅ {drug_name}, ho·∫∑c 'Th√¥ng tin c√°ch d√πng kh√¥ng c√≥ trong d∆∞·ª£c th∆∞ cho thu·ªëc n√†y.' n·∫øu kh√¥ng c√≥)",
  "notes": "Ph·∫ßn l∆∞u √Ω (CH·ªà th√¥ng tin c√≥ th·∫≠t trong PDF v·ªÅ {drug_name}, ho·∫∑c 'Th√¥ng tin l∆∞u √Ω kh√¥ng c√≥ trong d∆∞·ª£c th∆∞ cho thu·ªëc n√†y.' n·∫øu kh√¥ng c√≥)"
}}

**QUAN TR·ªåNG:**
- Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng th√™m text kh√°c
- KH√îNG ƒë∆∞·ª£c tr·∫£ v·ªÅ th√¥ng tin c·ªßa thu·ªëc kh√°c
- **TUY·ªÜT ƒê·ªêI KH√îNG B·ªäA RA TH√îNG TIN - CH·ªà T·ªîNG H·ª¢P TH√îNG TIN C√ì TH·∫¨T TRONG PDF**
- N·∫øu kh√¥ng t√¨m th·∫•y, ph·∫£i tr·∫£ v·ªÅ message "kh√¥ng c√≥ trong d∆∞·ª£c th∆∞" m·ªôt c√°ch r√µ r√†ng"""
        
        response = model.generate_content(prompt)
        result_text = response.text.strip()
        
        # Lo·∫°i b·ªè markdown code blocks n·∫øu c√≥
        result_text = result_text.replace('```json', '').replace('```', '').strip()
        
        # Parse JSON
        import json
        try:
            result = json.loads(result_text)
            usage = result.get('usage', '').strip()
            notes = result.get('notes', '').strip()
            
            # Ki·ªÉm tra xem c√≥ ph·∫£i l√† th√¥ng b√°o kh√¥ng c√≥ th√¥ng tin kh√¥ng
            # Chu·∫©n h√≥a message ƒë·ªÉ ƒë·∫£m b·∫£o r√µ r√†ng
            usage_lower = usage.lower()
            notes_lower = notes.lower()
            
            # Ki·ªÉm tra c√°c pattern cho "kh√¥ng c√≥ th√¥ng tin"
            no_info_patterns = [
                'kh√¥ng t√¨m th·∫•y',
                'kh√¥ng c√≥ trong',
                'kh√¥ng c√≥ th√¥ng tin',
                'ch∆∞a c√≥ th√¥ng tin',
                'thi·∫øu th√¥ng tin'
            ]
            
            if any(pattern in usage_lower for pattern in no_info_patterns):
                usage = "Th√¥ng tin c√°ch d√πng kh√¥ng c√≥ trong d∆∞·ª£c th∆∞ cho thu·ªëc n√†y."
            
            if any(pattern in notes_lower for pattern in no_info_patterns):
                notes = "Th√¥ng tin l∆∞u √Ω kh√¥ng c√≥ trong d∆∞·ª£c th∆∞ cho thu·ªëc n√†y."
            
            # Ki·ªÉm tra n·∫øu Gemini tr·∫£ v·ªÅ text qu√° ng·∫Øn ho·∫∑c kh√¥ng c√≥ √Ω nghƒ©a (c√≥ th·ªÉ l√† b·ªãa ra)
            # N·∫øu usage ho·∫∑c notes qu√° ng·∫Øn (< 20 k√Ω t·ª±) v√† kh√¥ng ph·∫£i l√† message "kh√¥ng c√≥", c√≥ th·ªÉ l√† l·ªói
            if len(usage.strip()) < 20 and not any(pattern in usage_lower for pattern in no_info_patterns):
                print(f"‚ö†Ô∏è Usage qu√° ng·∫Øn ({len(usage)} k√Ω t·ª±), c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c. ƒê·∫∑t l·∫°i th√†nh 'kh√¥ng c√≥'")
                usage = "Th√¥ng tin c√°ch d√πng kh√¥ng c√≥ trong d∆∞·ª£c th∆∞ cho thu·ªëc n√†y."
            
            if len(notes.strip()) < 20 and not any(pattern in notes_lower for pattern in no_info_patterns):
                print(f"‚ö†Ô∏è Notes qu√° ng·∫Øn ({len(notes)} k√Ω t·ª±), c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c. ƒê·∫∑t l·∫°i th√†nh 'kh√¥ng c√≥'")
                notes = "Th√¥ng tin l∆∞u √Ω kh√¥ng c√≥ trong d∆∞·ª£c th∆∞ cho thu·ªëc n√†y."
            
            # Gi·ªõi h·∫°n ƒë·ªô d√†i
            if len(usage) > 500:
                usage = usage[:500] + "..."
            if len(notes) > 600:
                notes = notes[:600] + "..."
            
            print(f"‚úÖ ƒê√£ t·ªïng h·ª£p th√¥ng tin v·ªõi Gemini cho {drug_name}")
            return {
                'usage': usage,
                'notes': notes
            }
        except json.JSONDecodeError:
            # N·∫øu kh√¥ng parse ƒë∆∞·ª£c JSON, th·ª≠ extract th·ªß c√¥ng
            print("‚ö†Ô∏è Kh√¥ng parse ƒë∆∞·ª£c JSON t·ª´ Gemini, th·ª≠ extract th·ªß c√¥ng")
            # T√¨m ph·∫ßn usage v√† notes trong text
            usage_start = result_text.find('"usage"') or result_text.find('C√ÅCH D√ôNG')
            notes_start = result_text.find('"notes"') or result_text.find('L∆ØU √ù')
            
            if usage_start > -1 and notes_start > -1:
                usage = result_text[usage_start:notes_start].replace('"usage":', '').strip('",')
                notes = result_text[notes_start:].replace('"notes":', '').strip('",')
                return {'usage': usage[:500], 'notes': notes[:600]}
            else:
                # Fallback: chia text l√†m 2 ph·∫ßn
                parts = result_text.split('\n\n')
                usage = parts[0] if len(parts) > 0 else ''
                notes = parts[1] if len(parts) > 1 else ''
                return {'usage': usage[:500], 'notes': notes[:600]}
        
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi g·ªçi Gemini API: {e}")
        return {'usage': '', 'notes': ''}

def generate_recommendations(drug_info, pdf_details):
    """
    T·∫°o khuy·∫øn ngh·ªã s·ª≠ d·ª•ng thu·ªëc d·ª±a tr√™n th√¥ng tin thu·ªëc
    """
    recommendations = []
    
    # Khuy·∫øn ngh·ªã d·ª±a tr√™n ph√¢n lo·∫°i
    category = drug_info.get('Category', '').lower()
    if 'kh√°ng sinh' in category:
        recommendations.append("Kh√°ng sinh c·∫ßn u·ªëng ƒë·ªß li·ªÅu v√† ƒë·ªß th·ªùi gian theo ch·ªâ ƒë·ªãnh c·ªßa b√°c sƒ©, kh√¥ng t·ª± √Ω ng·ª´ng thu·ªëc.")
    elif 'gi·∫£m ƒëau' in category or 'h·∫° s·ªët' in category:
        recommendations.append("Thu·ªëc gi·∫£m ƒëau h·∫° s·ªët n√™n u·ªëng sau khi ƒÉn ƒë·ªÉ tr√°nh k√≠ch ·ª©ng d·∫° d√†y.")
    elif 'ch·ªëng vi√™m' in category:
        recommendations.append("Thu·ªëc ch·ªëng vi√™m n√™n u·ªëng sau khi ƒÉn v√† u·ªëng nhi·ªÅu n∆∞·ªõc.")
    elif 'vitamin' in category or 'b·ªï sung' in category:
        recommendations.append("Vitamin v√† ch·∫•t b·ªï sung n√™n u·ªëng theo li·ªÅu l∆∞·ª£ng khuy·∫øn ngh·ªã, kh√¥ng l·∫°m d·ª•ng.")
    
    # Khuy·∫øn ngh·ªã d·ª±a tr√™n ch·ªëng ch·ªâ ƒë·ªãnh
    contraindications = pdf_details.get('contraindications', '').lower()
    if contraindications:
        if 'ph·ª• n·ªØ c√≥ thai' in contraindications or 'mang thai' in contraindications:
            recommendations.append("Kh√¥ng s·ª≠ d·ª•ng cho ph·ª• n·ªØ c√≥ thai ho·∫∑c ƒëang cho con b√∫ n·∫øu kh√¥ng c√≥ ch·ªâ ƒë·ªãnh c·ªßa b√°c sƒ©.")
        if 'tr·∫ª em' in contraindications or 'tr·∫ª nh·ªè' in contraindications:
            recommendations.append("C·∫ßn th·∫≠n tr·ªçng khi s·ª≠ d·ª•ng cho tr·∫ª em, n√™n tham kh·∫£o √Ω ki·∫øn b√°c sƒ©.")
    
    # Khuy·∫øn ngh·ªã d·ª±a tr√™n c√°ch d√πng
    usage = pdf_details.get('usage', '') or pdf_details.get('dosage', '')
    if usage:
        if 'sau khi ƒÉn' in usage.lower() or 'sau b·ªØa ƒÉn' in usage.lower():
            recommendations.append("N√™n u·ªëng thu·ªëc sau khi ƒÉn ƒë·ªÉ ƒë·∫°t hi·ªáu qu·∫£ t·ªët nh·∫•t v√† gi·∫£m t√°c d·ª•ng ph·ª•.")
        if 'tr∆∞·ªõc khi ƒÉn' in usage.lower() or 'khi ƒë√≥i' in usage.lower():
            recommendations.append("N√™n u·ªëng thu·ªëc tr∆∞·ªõc khi ƒÉn ho·∫∑c khi ƒë√≥i ƒë·ªÉ h·∫•p thu t·ªët h∆°n.")
    
    # Khuy·∫øn ngh·ªã chung
    if not recommendations:
        recommendations.append("Vui l√≤ng ƒë·ªçc k·ªπ h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng tr∆∞·ªõc khi d√πng v√† tu√¢n th·ªß li·ªÅu l∆∞·ª£ng khuy·∫øn ngh·ªã.")
        recommendations.append("N·∫øu c√≥ b·∫•t k·ª≥ d·∫•u hi·ªáu b·∫•t th∆∞·ªùng n√†o, h√£y ng·ª´ng s·ª≠ d·ª•ng v√† tham kh·∫£o √Ω ki·∫øn b√°c sƒ©.")
    else:
        recommendations.append("N·∫øu c√≥ b·∫•t k·ª≥ d·∫•u hi·ªáu b·∫•t th∆∞·ªùng n√†o, h√£y ng·ª´ng s·ª≠ d·ª•ng v√† tham kh·∫£o √Ω ki·∫øn b√°c sƒ©.")
    
    return recommendations

def extract_drug_details_from_pdf(page_number, offset=-1):
    """
    Tr√≠ch xu·∫•t th√¥ng tin chi ti·∫øt t·ª´ PDF d·ª±a tr√™n s·ªë trang
    T√¨m th√†nh ph·∫ßn, c√¥ng d·ª•ng, ch·ªâ ƒë·ªãnh, ch·ªëng ch·ªâ ƒë·ªãnh...
    """
    if pdf_reader is None:
        return {}
    
    try:
        # Chuy·ªÉn ƒë·ªïi s·ªë trang s√°ch th√†nh index PDF (pypdf ƒë√°nh s·ªë t·ª´ 0)
        # Offset th∆∞·ªùng l√† -1 v√¨ PDF c√≥ th·ªÉ c√≥ trang b√¨a, m·ª•c l·ª•c
        pdf_page_index = int(page_number) + offset - 1
        
        # ƒê·∫£m b·∫£o index h·ª£p l·ªá
        if pdf_page_index < 0 or pdf_page_index >= len(pdf_reader.pages):
            # Th·ª≠ kh√¥ng c√≥ offset
            pdf_page_index = int(page_number) - 1
            if pdf_page_index < 0 or pdf_page_index >= len(pdf_reader.pages):
                return {}
        
        # ƒê·ªçc trang PDF
        page = pdf_reader.pages[pdf_page_index]
        text = page.extract_text()
        
        if not text:
            return {}
        
        # T√¨m ph·∫ßn text li√™n quan ƒë·∫øn thu·ªëc c·ª• th·ªÉ (n·∫øu c√≥ t√™n thu·ªëc trong text)
        # L·∫•y to√†n b·ªô text nh∆∞ng s·∫Ω filter trong prompt c·ªßa Gemini
        full_text = text
        
        details = {
            'composition': '',
            'indications': '',
            'contraindications': '',
            'dosage': '',
            'usage': '',  # C√°ch d√πng
            'full_text': full_text  # Gi·ªØ to√†n b·ªô text ƒë·ªÉ Gemini c√≥ th·ªÉ filter
        }
        
        # Regex patterns ƒë·ªÉ t√¨m c√°c th√¥ng tin
        patterns = {
            'composition': re.compile(r'(Th√†nh ph·∫ßn|Th√†nh ph·∫ßn ch√≠nh|Ho·∫°t ch·∫•t)[:\.]\s*(.+?)(?:\n|$)', re.IGNORECASE),
            'indications': re.compile(r'(Ch·ªâ ƒë·ªãnh|C√¥ng d·ª•ng|T√°c d·ª•ng)[:\.]\s*(.+?)(?:\n|Ch·ªëng ch·ªâ ƒë·ªãnh|Li·ªÅu d√πng|$)', re.IGNORECASE | re.DOTALL),
            'contraindications': re.compile(r'(Ch·ªëng ch·ªâ ƒë·ªãnh|Kh√¥ng d√πng)[:\.]\s*(.+?)(?:\n|Li·ªÅu d√πng|C√°ch d√πng|$)', re.IGNORECASE | re.DOTALL),
            'dosage': re.compile(r'(Li·ªÅu d√πng|C√°ch d√πng|Li·ªÅu l∆∞·ª£ng|C√°ch s·ª≠ d·ª•ng)[:\.]\s*(.+?)(?:\n|T√°c d·ª•ng ph·ª•|L∆∞u √Ω|B·∫£o qu·∫£n|$)', re.IGNORECASE | re.DOTALL),
            'usage': re.compile(r'(C√°ch d√πng|H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng|S·ª≠ d·ª•ng)[:\.]\s*(.+?)(?:\n|L∆∞u √Ω|T√°c d·ª•ng ph·ª•|$)', re.IGNORECASE | re.DOTALL)
        }
        
        # T√¨m t·ª´ng lo·∫°i th√¥ng tin
        for key, pattern in patterns.items():
            match = pattern.search(text)
            if match:
                details[key] = match.group(2).strip()[:500]  # Gi·ªõi h·∫°n ƒë·ªô d√†i
        
        # N·∫øu kh√¥ng t√¨m th·∫•y "usage", d√πng "dosage" l√†m c√°ch d√πng
        if not details['usage'] and details['dosage']:
            details['usage'] = details['dosage']
        
        return details
        
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói ƒë·ªçc PDF trang {page_number}: {e}")
        return {}

@app.route('/api/health', methods=['GET', 'OPTIONS'])
def health_check():
    """Health check endpoint"""
    # Handle CORS preflight
    if request.method == 'OPTIONS':
        response = jsonify({})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Methods', 'GET, OPTIONS')
        response.headers.add('Access-Control-Allow-Headers', 'Content-Type')
        return response
    
    return jsonify({
        'status': 'ok',
        'message': 'Backend API is running',
        'drugs_loaded': len(drug_db) if drug_db is not None else 0
    })

@app.route('/api/scan', methods=['POST', 'OPTIONS'])
def scan_drug():
    """API endpoint ƒë·ªÉ scan thu·ªëc t·ª´ ·∫£nh ho·∫∑c text"""
    # Handle CORS preflight
    if request.method == 'OPTIONS':
        response = jsonify({})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Methods', 'POST, OPTIONS')
        response.headers.add('Access-Control-Allow-Headers', 'Content-Type')
        return response
    
    try:
        # Ki·ªÉm tra request c√≥ JSON kh√¥ng
        if not request.is_json:
            print("‚ùå Request kh√¥ng ph·∫£i JSON")
            return jsonify({
                'success': False,
                'error': 'Invalid request format',
                'message': 'Request ph·∫£i l√† JSON format'
            }), 400
        
        # Ki·ªÉm tra xem c√≥ text ƒë∆∞·ª£c g·ª≠i tr·ª±c ti·∫øp kh√¥ng (t·ª´ modal x√°c nh·∫≠n OCR)
        if request.json and 'text' in request.json:
            confirmed_text = request.json['text']
            print(f"üìù T√¨m ki·∫øm v·ªõi text ƒë√£ x√°c nh·∫≠n: {confirmed_text}")
            
            # T√¨m ki·∫øm trong database
            drug_info = search_drug_in_database(confirmed_text, None)
            
            if drug_info:
                # Ki·ªÉm tra thu·ªëc k√™ ƒë∆°n
                is_prescription = drug_info.get('Is_Prescription', False)
                if isinstance(is_prescription, str):
                    is_prescription = is_prescription.lower() in ['true', '1', 'yes']
                elif pd.isna(is_prescription):
                    is_prescription = False
                
                if is_prescription:
                    return jsonify({
                        'success': False,
                        'error': 'PRESCRIPTION_REQUIRED',
                        'message': '‚ö†Ô∏è ƒê√¢y l√† thu·ªëc k√™ ƒë∆°n. Vui l√≤ng s·ª≠ d·ª•ng theo ch·ªâ ƒë·ªãnh c·ªßa b√°c sƒ©.',
                        'drug_name': drug_info.get('DrugName', ''),
                        'active_ingredient': drug_info.get('ActiveIngredient', ''),
                        'category': drug_info.get('Category', ''),
                        'extracted_text': confirmed_text
                    }), 403
                
                # L·∫•y th√¥ng tin t·ª´ PDF
                page_number = drug_info.get('PageNumber', '')
                pdf_details = {}
                if page_number and pdf_reader:
                    pdf_details = extract_drug_details_from_pdf(page_number)
                    
                    # S·ª≠ d·ª•ng Gemini ƒë·ªÉ t·ªïng h·ª£p th√¥ng tin t·ª´ PDF
                    drug_name = drug_info.get('DrugName', '')
                    pdf_full_text = pdf_details.get('full_text', '')
                    
                    if pdf_full_text:
                        # T·ªïng h·ª£p v·ªõi Gemini: c√°ch d√πng + l∆∞u √Ω
                        gemini_summary = summarize_drug_info_with_gemini(pdf_full_text, drug_name, drug_info)
                        
                        # C·∫≠p nh·∫≠t usage v√† th√™m notes
                        if gemini_summary.get('usage'):
                            pdf_details['usage'] = gemini_summary['usage']
                        if gemini_summary.get('notes'):
                            pdf_details['notes'] = gemini_summary['notes']
                
                # T·∫°o khuy·∫øn ngh·ªã
                recommendations = generate_recommendations(drug_info, pdf_details)
                
                return jsonify({
                    'success': True,
                    'drug_name': drug_info.get('DrugName', ''),
                    'active_ingredient': drug_info.get('ActiveIngredient', ''),
                    'page_number': str(page_number),
                    'category': drug_info.get('Category', ''),
                    'extracted_text': confirmed_text,
                    'rx_status': 'OTC',
                    'composition': pdf_details.get('composition', ''),
                    'indications': pdf_details.get('indications', ''),
                    'contraindications': pdf_details.get('contraindications', ''),
                    'dosage': pdf_details.get('dosage', ''),
                    'usage': pdf_details.get('usage', ''),  # C√°ch d√πng (t·ªïng h·ª£p b·ªüi Gemini)
                    'notes': pdf_details.get('notes', ''),  # L∆∞u √Ω (t·ªïng h·ª£p b·ªüi Gemini)
                    'recommendations': recommendations  # Khuy·∫øn ngh·ªã
                })
            else:
                return jsonify({
                    'success': False,
                    'message': 'Kh√¥ng t√¨m th·∫•y th√¥ng tin thu·ªëc trong database',
                    'extracted_text': confirmed_text
                }), 404
        
        # Ki·ªÉm tra xem c√≥ file ho·∫∑c base64 image kh√¥ng
        if 'image' in request.files:
            # Nh·∫≠n file upload
            file = request.files['image']
            if file and allowed_file(file.filename):
                # ƒê·ªçc ·∫£nh t·ª´ file
                image_bytes = file.read()
                image = Image.open(io.BytesIO(image_bytes))
                if image.mode != 'RGB':
                    image = image.convert('RGB')
                image_array = np.array(image)
        elif 'image' in request.json:
            # Nh·∫≠n base64 image
            base64_image = request.json['image']
            
            # Lo·∫°i b·ªè data URL prefix n·∫øu c√≥ (data:image/jpeg;base64,...)
            if ',' in base64_image:
                base64_image = base64_image.split(',')[1]
            
            image_array = decode_base64_image(base64_image)
            if image_array is None:
                print(f"‚ùå L·ªói decode base64 image. Length: {len(base64_image) if base64_image else 0}")
                return jsonify({
                    'success': False,
                    'error': 'Invalid image data',
                    'message': 'Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh. Vui l√≤ng th·ª≠ l·∫°i v·ªõi ·∫£nh kh√°c.'
                }), 400
        else:
            print(f"‚ùå Kh√¥ng c√≥ image trong request. Keys: {list(request.json.keys()) if request.json else 'No JSON'}")
            return jsonify({
                'success': False,
                'error': 'No image provided',
                'message': 'Vui l√≤ng cung c·∫•p ·∫£nh ƒë·ªÉ qu√©t.'
            }), 400
        
        # Ti·ªÅn x·ª≠ l√Ω ·∫£nh
        processed_image = preprocess_image(image_array)
        
        # Tr√≠ch xu·∫•t text t·ª´ ·∫£nh (OCR) - tr·∫£ v·ªÅ text ƒë√£ ch·ªçn v√† t·∫•t c·∫£ text
        extracted_text, all_ocr_texts = extract_text_from_image(image_array)  # D√πng ·∫£nh g·ªëc
        
        # Ki·ªÉm tra k·∫øt qu·∫£ OCR
        if extracted_text is None:
            print("‚ùå OCR tr·∫£ v·ªÅ None")
            return jsonify({
                'success': False,
                'message': 'L·ªói khi x·ª≠ l√Ω OCR. Vui l√≤ng th·ª≠ l·∫°i.',
                'extracted_text': '',
                'all_ocr_texts': all_ocr_texts or []
            }), 500
        
        if not extracted_text or extracted_text.strip() == '':
            print(f"‚ö†Ô∏è OCR kh√¥ng t√¨m th·∫•y text. All texts: {all_ocr_texts}")
            # V·∫´n tr·∫£ v·ªÅ 200 nh∆∞ng v·ªõi success=False ƒë·ªÉ frontend c√≥ th·ªÉ x·ª≠ l√Ω
            return jsonify({
                'success': False,
                'message': 'Kh√¥ng th·ªÉ nh·∫≠n di·ªán text t·ª´ ·∫£nh. Vui l√≤ng th·ª≠ l·∫°i v·ªõi ·∫£nh r√µ h∆°n ho·∫∑c ch·ª•p l·∫°i.',
                'extracted_text': '',
                'all_ocr_texts': all_ocr_texts or []
            }), 200  # ƒê·ªïi th√†nh 200 ƒë·ªÉ frontend c√≥ th·ªÉ x·ª≠ l√Ω
        
        print(f"üìù Text nh·∫≠n di·ªán ƒë∆∞·ª£c: {extracted_text}")
        print(f"üìã T·∫•t c·∫£ text OCR: {all_ocr_texts}")
        
        # T√¨m ki·∫øm trong database - truy·ªÅn c·∫£ all_ocr_texts ƒë·ªÉ t√¨m v·ªõi c√°c text kh√°c
        drug_info = search_drug_in_database(extracted_text, all_ocr_texts)
        
        if drug_info:
            # KI·ªÇM TRA AN TO√ÄN: N·∫øu l√† thu·ªëc k√™ ƒë∆°n (Is_Prescription = True), ch·∫∑n l·∫°i
            is_prescription = drug_info.get('Is_Prescription', False)
            
            # Chuy·ªÉn ƒë·ªïi gi√° tr·ªã boolean t·ª´ CSV (c√≥ th·ªÉ l√† string "True"/"False" ho·∫∑c boolean)
            if isinstance(is_prescription, str):
                is_prescription = is_prescription.lower() in ['true', '1', 'yes']
            elif pd.isna(is_prescription):
                is_prescription = False
            
            if is_prescription:
                return jsonify({
                    'success': False,
                    'error': 'PRESCRIPTION_REQUIRED',
                    'message': '‚ö†Ô∏è ƒê√¢y l√† thu·ªëc k√™ ƒë∆°n. Vui l√≤ng s·ª≠ d·ª•ng theo ch·ªâ ƒë·ªãnh c·ªßa b√°c sƒ©.',
                    'drug_name': drug_info.get('DrugName', ''),
                    'active_ingredient': drug_info.get('ActiveIngredient', ''),
                    'category': drug_info.get('Category', ''),
                    'extracted_text': extracted_text,
                    'all_ocr_texts': all_ocr_texts  # Tr·∫£ v·ªÅ t·∫•t c·∫£ text OCR
                }), 403  # 403 Forbidden
            
            # N·∫øu l√† thu·ªëc OTC, ti·∫øp t·ª•c tra c·ª©u th√¥ng tin chi ti·∫øt t·ª´ PDF
            page_number = drug_info.get('PageNumber', '')
            pdf_details = {}
            
            if page_number and pdf_reader:
                pdf_details = extract_drug_details_from_pdf(page_number)
                
                # S·ª≠ d·ª•ng Gemini ƒë·ªÉ t·ªïng h·ª£p th√¥ng tin t·ª´ PDF
                drug_name = drug_info.get('DrugName', '')
                pdf_full_text = pdf_details.get('full_text', '')
                
                if pdf_full_text:
                    # T·ªïng h·ª£p v·ªõi Gemini: c√°ch d√πng + l∆∞u √Ω
                    gemini_summary = summarize_drug_info_with_gemini(pdf_full_text, drug_name, drug_info)
                    
                    # C·∫≠p nh·∫≠t usage v√† th√™m notes
                    if gemini_summary.get('usage'):
                        pdf_details['usage'] = gemini_summary['usage']
                    if gemini_summary.get('notes'):
                        pdf_details['notes'] = gemini_summary['notes']
            
            # T·∫°o khuy·∫øn ngh·ªã
            recommendations = generate_recommendations(drug_info, pdf_details)
            
            # Tr·∫£ v·ªÅ th√¥ng tin thu·ªëc ƒë·∫ßy ƒë·ªß
            return jsonify({
                'success': True,
                'drug_name': drug_info.get('DrugName', ''),
                'active_ingredient': drug_info.get('ActiveIngredient', ''),
                'page_number': str(page_number),
                'category': drug_info.get('Category', ''),
                'extracted_text': extracted_text,
                'all_ocr_texts': all_ocr_texts,  # Tr·∫£ v·ªÅ t·∫•t c·∫£ text OCR
                'rx_status': 'OTC',
                'composition': pdf_details.get('composition', ''),
                'indications': pdf_details.get('indications', ''),
                'contraindications': pdf_details.get('contraindications', ''),
                'dosage': pdf_details.get('dosage', ''),
                'usage': pdf_details.get('usage', ''),  # C√°ch d√πng (t·ªïng h·ª£p b·ªüi Gemini)
                'notes': pdf_details.get('notes', ''),  # L∆∞u √Ω (t·ªïng h·ª£p b·ªüi Gemini)
                'recommendations': recommendations  # Khuy·∫øn ngh·ªã
            })
        else:
            # Kh√¥ng t√¨m th·∫•y trong database - tr·∫£ v·ªÅ ƒë·ªÉ user c√≥ th·ªÉ x√°c nh·∫≠n OCR
            return jsonify({
                'success': False,
                'needs_ocr_confirm': True,  # Flag ƒë·ªÉ frontend hi·ªÉn th·ªã modal x√°c nh·∫≠n
                'message': 'Kh√¥ng t√¨m th·∫•y th√¥ng tin thu·ªëc trong database',
                'extracted_text': extracted_text,
                'all_ocr_texts': all_ocr_texts  # Tr·∫£ v·ªÅ t·∫•t c·∫£ text OCR
            }), 404
            
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        print(f"‚ùå Error processing scan: {e}")
        print(f"üìã Traceback:\n{error_trace}")
        return jsonify({
            'success': False,
            'error': 'Internal server error',
            'message': f'L·ªói khi x·ª≠ l√Ω: {str(e)}'
        }), 500

@app.route('/api/drugs/search', methods=['GET'])
def search_drugs():
    """API endpoint ƒë·ªÉ t√¨m ki·∫øm thu·ªëc theo t√™n"""
    query = request.args.get('q', '')
    if not query:
        return jsonify({'error': 'Query parameter required'}), 400
    
    if drug_db is None or drug_db.empty:
        return jsonify({'drugs': []})
    
    # T√¨m ki·∫øm
    query_lower = query.lower()
    results = drug_db[drug_db['DrugName'].str.lower().str.contains(query_lower, na=False)]
    
    # Gi·ªõi h·∫°n k·∫øt qu·∫£
    results = results.head(20)
    
    return jsonify({
        'drugs': results.to_dict('records')
    })

if __name__ == '__main__':
    load_drug_database()
    load_pdf()
    print("üöÄ Starting MediScan AI Backend Server...")
    print("üì° API available at http://localhost:5000")
    
    # Hi·ªÉn th·ªã IP local ƒë·ªÉ k·∫øt n·ªëi t·ª´ mobile
    try:
        import socket
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        s.connect(("8.8.8.8", 80))
        local_ip = s.getsockname()[0]
        s.close()
        print(f"üì± Mobile access: http://{local_ip}:5000")
        print(f"   (ƒê·∫£m b·∫£o mobile v√† m√°y t√≠nh c√πng WiFi)")
    except:
        pass
    
    app.run(debug=True, host='0.0.0.0', port=5000)

